{
  "hash": "1d4d61e8fa0d77f7d22146876ce2681c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Interpreting the confidence interval in front of you\nauthor: Josue Rodriguez\ndescription: |\n I go over the most common interpretation of a confidence interval, and discuss an alternative, potentially more useful one.\ndate: \"2024-12-16\"\nformat:\n  html:\n    fig-align: center\n    include-before-body: ../../resources/mathjax.tex\n    toc: true\n    code-fold: true\nfrom: markdown+emoji\nbibliography: refs.bib\ncsl: ../../resources/apa.csl\ncategories: [statistics, uncertainty]\n---\n\n\n\n# Takeaway\n\n**A confidence interval contains the parameter values that are consistent with your observed sample data.**\n\nFor example, suppose we observe a sample mean $\\bar{x} = 100$ and calculate the $95\\%$ confidence interval for the population mean to be $[90, 110]$. This interval suggests that any population mean between $90$ and $110$ could plausibly lead to observing a sample mean of $100$.\n\nThe main point of this post is that the above interpretation of a confidence interval is a valid one, and that it is more useful than the conventional interpretation. If you'd like to better understand what the above interpretation means and why it's valid, I invite you to read on. \n\n___\n\n# Introduction\n\nConfidence intervals are a cornerstone of statistical inference; they're used to express uncertainty around a point estimate. However, misinterpretations of confidence intervals abound. This is both unsurprising and understandable since 1) confidence intervals are usually defined in reference to an infinite series of hypothetical, repeated experiments, and 2) most people analyzing data are usually concerned with a single experiment. \n\nConfidence intervals aren't fundamentally incompatible with analysts' goals, though. They can be interpreted in a way that is meaningful (and potentially even useful!) in the context of a single experiment, and the goal of this post is to walk through such an interpretation with words, code, and a bit of math. \n\n## Set up\n\nSuppose we sample $x_1, \\dots, x_n$ from a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$, and that we are interested in testing the follow hypotheses about the population mean\n\n\n$$\n\\begin{align}\nH_0{:\\;}\\mu &= \\theta \\\\\n& \\text{vs.} \\\\\nH_1{:\\;} \\mu &\\neq \\theta,\n\\end{align}\n$$\n\nfor some $\\theta$[^1]. We can choose a significance level $\\alpha$ and perform a two-sided $t$-test to assess this null hypothesis. Recall that our $\\alpha$ level is our error rate -- the probability that we reject the null hypothesis when the null hypothesis is true. Canonically, $\\theta = 0$ and $\\alpha = 0.05$.\n\n[^1]: I'll focus on hypothesis tests concerning populuation means in this post, but the arguments are applicable to other tests as well, e.g., those concerning mean differences or regression coefficients.\n\nWe first find our critical value for this test, $t_{crit, \\alpha}$. We get this value by finding the $1 - \\alpha/2$ quantile of the $\\text{Student-}t$ distribution with $n-1$ degrees of freedom. \n\nWe can then calculate our observed $t$-statistic as\n\n$$\nt_{obs} = \\frac{\\bar{x} - \\theta}{s/\\sqrt{n}}\n$$\n\nwhere $\\bar{x}$ denotes the sample mean and $s$ denotes the sample standard deviation.\n\nWe reject the null hypothesis that $\\mu = \\theta$ when the $p$-value is greater than $\\alpha$, or $|t_{obs}| \\geq t_{crit, \\alpha}.$ Conversely, we fail to reject the null hypothesis whenever the $p$-value is less than $\\alpha$, or $|t_{obs}| < t_{crit, \\alpha}$.\n\n___\n\n# The \"usual\" interpretation of a CI\n\n\nIt's often desired to have a measure of uncertainty to accompany the point estimate in the form of a confidence interval.\n\nA $(1 - \\alpha)100\\%$ CI is an interval that covers the true parameter value in approximately $(1 - \\alpha)100\\%$ of repeated experiments. Such an interval is a function of our sample data and can be constructed as \n\n$$\nC(x_1, \\dots, x_n) = \\left[\\bar{x} - \\frac{s}{\\sqrt{n}}t_{crit, \\alpha},~ \\bar{x} + \\frac{s}{\\sqrt{n}}t_{crit, \\alpha} \\right].\n$$\n\n\nTo make the interpretation a bit more concrete, let's simulate 100 experiments at $\\alpha = 0.05$ and compute a $95\\%$ confidence interval for each. We should expect approximately 95 of the 100 intervals to cover the true parameter value. \n\nThe code below simulates these experiments by repeatedly drawing random samples from a normal distribution $\\mathcal{N}(50, 20^2)$ and calculating a confidence interval for the population mean. The resulting confidence intervals are then plotted. They are colored blue if they contain the the population mean and red if they do not.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(271828)\n\n#' compute the confidence interval for the mean of x\n#' @param x sample data\n#' @param alpha  the significance level\ncompute_ci <- function(x, alpha = 0.05) {\n  xbar <- mean(x)\n  std_err <- sd(x) / sqrt(n)\n  n <- length(x)\n\n  tcrit <- qt(1 - (alpha / 2), df = n - 1)\n\n  lb <- xbar - tcrit * std_err\n  ub <- xbar + tcrit * std_err\n\n  return(c(lb = lb, ub = ub))\n}\n\n# define population values\nmu <- 50\nstd <- 20\n\n# sample size\nn <- 100\n\nn_experiments <- 100\n\n# run \"experiments\"\nci_df <- purrr::map_dfr(\n  1:n_experiments, \n  function(i) compute_ci(rnorm(n, mu, std))\n)\n\n# determine whether each CI contains mu and plot\nci_covers_df <- ci_df %>%\n  tibble::rowid_to_column() %>%\n  mutate(covers = ifelse(lb <= mu & mu <= ub, \"yes\", \"no\"))\n\nn_covered <- sum(ci_covers_df$covers == \"yes\")\n\nci_covers_df %>%\n  ggplot(aes(rowid)) +\n  geom_errorbar(aes(ymin = lb, ymax = ub, col = covers)) +\n  geom_hline(yintercept = mu) +\n  theme_minimal(base_size = 16) +\n  guides(\n    col = guide_legend(\n      title = \"CI includes population mean\", \n      position = \"top\", \n      title.position = \"top\",\n      title.hjust = 0.5\n    )\n  ) +\n  labs(\n    x = \"Experiment Number\",\n    y = \"CI Values\"\n  ) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\nIn this example, exactly  95 out of 100 $95\\%$ CIs cover the population mean of 50. Exactly as we would expect for a confidence interval.\n\nIn practice, though, you usually run one experiment and so you only get one confidence interval. [A common misunderstanding leads analysts to say something like](https://en.wikipedia.org/wiki/Confidence_interval#Common_misunderstandings) \"there's a $95\\%$ chance that the population parameter lies within the interval.\", but that's not quite right. Once you've computed a CI it either covers the population mean or it does not[^2].\n\n[^2]:Julia Rohrer says the following on [her blog](https://www.the100.ci/2024/12/05/why-you-are-not-allowed-to-say-that-your-95-confidence-interval-contains-the-true-parameter-with-a-probability-of-95/), though, and I do think it's a point well-worth considering\n\n    > What are the downstream consequences of rampant misinterpretation of confidence intervals? As far as I can tell, the whole exercise seems mostly concerned about language. If I say \"with a probability of 95%, this interval contains the true parameter\", I commit a faux pas. If I say \"here’s my interval; in the long run, 95% of intervals created in this manner contain the true value\", I exhibit technical sophistication. But will any of my downstream inferences look differently? Is any consumer of my findings going to deal with the information differently?\n\nBecause the usual interpretation of a CI relies on the notion of long-run frequencies we *could* say \"[Were this procedure to be repeated on numerous samples, the proportion of calculated 95% confidence intervals that encompassed the true [population mean] would tend toward 95%\"](https://en.wikipedia.org/wiki/Confidence_interval#Interpretation). But this really doesn't seem helpful in interpreting a confidence interval based on a single sample, so what can we say instead?\n\n___\n\n# An alternative interpretation of a CI\n\nYou might recall a statistics professor telling you that if 0 is in your confidence interval, then you fail to reject the null hypothesis that $\\mu = 0$. You might even recall asking yourself why that is the case. This idea is based on an alternative interpretation of a confidence interval.  To be clear, the \"usual\" interpretation we saw above isn't wrong, it's simply one perspective. Here, we'll explore the concept from a different angle.\n\n[Wikipedia offers the following interpretation of a confidence interval](https://en.wikipedia.org/wiki/Confidence_interval#Interpretation)\n\n> The confidence interval can be expressed in terms of statistical significance, e.g.: \"The 95% confidence interval represents values that are not statistically significantly different from the point estimate at the .05 level.\"\n\nEssentially, this says that if we set up a null hypothesis $H_0{:\\;} \\mu = \\theta$ such that $\\theta$ is any value covered by the $(1-\\alpha)100\\%$ confidence interval, then we would fail to reject the null hypothesis at a significance level of $\\alpha$.  Importantly for us, notice that this interpretation doesn't make any references to a series of hypothetical experiments, but references only a single confidence interval. \n\nTo better grasp this alternative interpretation, it will again be helpful to walk through an example. Let's start by generating sample data and computing a $(1-\\alpha)100\\%$ confidence interval for the mean. We'll stick with $\\alpha = 0.05$ so that we get a $95\\%$ CI.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# n = 100, mu = 1, std = 1\nx <- rnorm(n, mu, std)\nci <- compute_ci(x, alpha = 0.05)\n\nround(mean(x), 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 53.11\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nround(ci, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   lb    ub \n49.44 56.78 \n```\n\n\n:::\n:::\n\n\n\nOkay, so we've collected some data and observed a sample mean of 53.11 and computed a confidence interval goes from 49.44 to 56.78. \n\nWhat would happen if we set up a hypothesis test $H_0{:\\;} \\mu  = 49.5$? Or if instead we set up $H_0{:\\;} \\mu  = 57$? According to Wikipedia, we would fail to reject the first null hypothesis and reject the second because 49.5 is inside the interval and 57 is not. If we run those $t$-tests and extract the $p$-values, we see that's exactly what would happen.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nalpha  <- 0.05\nt.test(x, mu = 49.5, conf.level = 1 - alpha)$p.value\nt.test(x, mu = 57, conf.level = 1 - alpha)$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05410432\n[1] 0.03778379\n```\n\n\n:::\n:::\n\n\n\nJust to be sure, let's expand on the code above such that we test $H_0{:\\;} \\mu = \\theta$ for a range of values both inside and outside of the confidence interval and see what happens.\n\n\n\n::: {.cell .preview-image layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# generate a range of thetas inside & outside the CI\nthetas <- seq(ci[\"lb\"] - 3, ci[\"ub\"] + 3, length.out = 1000)\n\n#' compute the p-value for a one-sample t-test\n#' @param x sample data\n#' @param theta the value of theta in the null hypothesis\n#' @param alpha the significance level\nget_t_test_pval <- function(x, theta, alpha = 0.05) {\n  res <- t.test(x, mu = theta, conf.level = 1 - alpha)\n  return(res$p.val)\n}\n\n# for every theta in thetas, run a hypothesis test for\n# H_O: mu = theta and get the p-value\npvalues <- sapply(thetas, function(theta) get_t_test_pval(x, theta))\n\npvalues_df <- tibble(thetas, pvalues) %>%\n  mutate(reject_h0 = ifelse(pvalues < .05, \"yes\", \"no\"))\n\nggplot(pvalues_df, aes(thetas, pvalues)) +\n  geom_col(aes(fill = reject_h0)) +\n  ggsci::scale_fill_jama() +\n  geom_errorbarh(aes(y = 0.02, xmin = ci[1], xmax = ci[2]), height = 0, col = \"white\", lwd = 1) +\n  geom_point(x = mean(x), y = 0.02, col = \"white\", size = 3) +\n  theme_minimal(base_size = 16) +\n  guides(\n    fill = guide_legend(\n      position = \"top\",\n      title = latex2exp::TeX(\"Reject $H_0 : \\\\; \\\\mu = \\\\theta$\"),\n      title.position = \"top\",\n      title.hjust = 0.5\n    )\n  ) + \n  labs(\n    x = expression(theta),\n    y = \"p-value\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\nThe x-axis in the above plot reflects the range of the different $\\theta$ values and the y-axis reflects the $p$-value for each corresponding $t$-test. The white point corresponds the sample mean of 53.11 and the white horizontal line corresponds to the confidence interval [49.44, 56.78.]. \n\nJust like Wikipedia claimed! None of the values inside of the confidence interval are statistically significantly different from the sample mean since their $p$-values are greater than or equal to .05. **In other words, each value in the confidence interval represents a value for $\\mu$ that is consistent with the observed sample mean**. On the other hand, all $p$-values for the tests concerning $\\theta$ values outside of the interval are $<.05$. By the same logic, each value outside the confidence interval represents a value for $\\mu$ that is *in*-consistent with the observed sample mean.\n\n___\n\nSo we have managed to find a way to say something about the values inside of a single confidence interval -- they are parameter values that are not significantly different than the sample mean -- but can we be a little more precise?\n\nI think so. We can clarify what is meant by \"consistent\" in this context. To start, let's revisit a common interpretation of a $p$-value. It's the probability of obtaining test results at least as extreme as the results we actually observed, under the assumption that the null hypothesis is correct. The phrase \"at least as extreme\" can be read as \"greater than or equal to the absolute value of\". We fail to reject $H_0{:\\;} \\mu = \\theta$ when the $p$-value is less than $\\alpha$. \n\nFor example, if the population mean was truly $\\mu = 49.5$, then the probability of observing a sample mean $|\\bar{x}| \\geq 53.11$ is 0.054 (this is the $p$-value we computed above). In other words, a population mean of 49.5 is consistent with our observed sample mean because if it really was the true population mean, we would observe a sample mean $|\\bar{x}| > 53.11$ 5.4% of the time. The reason 49.5 is \"consistent\" with our observed sample mean, is because the $p$-value of 5.4% is greater than 5%, the $\\alpha$ level.\n\nWe can extend this logic to all of the parameter values inside the confidence interval to say how the values in the confidence interval are consistent with the sample mean. **Each value in a $(1 - \\alpha)100\\%$ confidence interval represents a value for $\\mu$ that would lead to observing a sample mean greater than or equal to $|\\bar{x}|$ at least $\\alpha(100)\\%$ of the time**[^3].\n\n[^3]: The $p$-values are exactly 0.05 for the lower and upper bounds of the interval, and increase to 1 as you move toward the middle of the CI to the sample mean.\n\nThe data we generated above had a sample mean of 53.11 and a 95% CI with lower and upper bounds of 49.44 and 56.78, respectively. We can say that the the values in the CI are consistent with the sample mean in that we would expect to see a sample mean greater than or equal to 53.11 at least 5% of the time if the true population mean $\\mu$ was equal to any value between 49.44 and 56.78.\n\n\n### A Caveat\n\nThis exercise has allowed us to interpret a given confidence interval without making reference to the long-run proportion of times a confidence interval covers the population parameter in an infinite series of hypothetical experiments. What this exercise hasn't done is help us determine whether the confidence interval which we've computed covers the population parameter ([see the plot above](#the-usual-interpretation-of-a-ci)). Thus, we should still expect to completely miss the population parameter in $\\alpha(100)\\%$ of CIs over the long run, as $\\alpha$ represents our error rate.\n\n___\n\n# Why does this work?\n\nTo understand why values inside of the confidence interval are not statistically significantly different from the point estimate, we'll need to consider the relationship between hypothesis tests and confidence intervals. Recall that we are testing hypotheses regarding a population mean $H_0{:\\;} \\mu = \\theta \\text{ vs. } H_1{:\\;} \\mu \\neq \\theta$ at significance level $\\alpha$, and we fail to reject $H_0$ whenever\n\n$$\nt_{obs} < t_{crit, \\alpha}.\n$$\n\nWe can expand on that inequality to see that we fail to reject $H_0$ whenever\n\n$$\n\\begin{align}\n-t_{crit, \\alpha} <&  t_{obs} < t_{crit, \\alpha} \\\\\n-t_{crit, \\alpha} <& \\frac{\\bar{x} - \\theta}{s/\\sqrt{n}} < t_{crit, \\alpha} \\\\\n-t_{crit, \\alpha}\\frac{s}{\\sqrt{n}} <& \\bar{x} - \\theta < t_{crit, \\alpha} \\frac{s}{\\sqrt{n}} \\\\\n-\\bar{x}-t_{crit, \\alpha}\\frac{s}{\\sqrt{n}} <&  - \\theta < -\\bar{x} + t_{crit, \\alpha} \\frac{s}{\\sqrt{n}} \\\\\n\\bar{x}-t_{crit, \\alpha}\\frac{s}{\\sqrt{n}} &<  \\theta < \\bar{x} + t_{crit, \\alpha} \\frac{s}{\\sqrt{n}}.\n\\end{align}\n$$\n\nThere's three things I'd like to point out here:\n\n1. The bounds in the final expression should look familar. They are the same bounds as a $(1 - \\alpha)100\\%$ confidence interval.\n\n2. Which means that we fail to reject $H_0{:\\;} \\mu = \\theta$ whenever $\\theta$ is covered by the confidence interval. Just like we saw earlier.\n\n3. There is a direct connection between a hypothesis test for $\\theta$ and a confidence interval. A confidence interval can be obtained by \"inverting\" a hypothesis test.\n\n\nTo further explore the relationship between hypothesis tests and confidence intervals, we need to introduce the notion of an acceptance region.\n\nA hypothesis test for $H_0{:\\;} \\mu = \\theta$ has a so-called \"acceptance region\". It's defined as the set in the *sample space* for which we don't reject $H_0$\n\n$$\nA(\\theta) = \\left\\{(x_1, \\dots, x_n): \\theta - t_{crit, \\alpha} \\frac{s}{\\sqrt{n}}  < \\bar{x} < \\theta + t_{crit, \\alpha} \\frac{s}{\\sqrt{n}} \\right\\}.\n$$\n\nWe fail to reject $H_0{:\\;} \\mu = \\theta$ any time we observe a sample that's consistent with $\\mu = \\theta$. That is, when $x_1, \\dots, x_n$ belong to the acceptance region for $\\theta$.\n\n\nA confidence interval is the set in the *parameter* space with plausible values of $\\theta$\n\n$$\nC(x_1, \\dots, x_n) = \\left\\{\\theta: \\bar{x} - t_{crit, \\alpha} \\frac{s}{\\sqrt{n}}  < \\theta < \\bar{x} + t_{crit, \\alpha} \\frac{s}{\\sqrt{n}} \\right\\}.\n$$\n\nAs we've seen a few times now, we fail to reject $H_0{:\\;} \\mu = \\theta$ any time $\\theta$ is consistent with $x_1,\\dots, x_n$. That is, when $\\theta$ is in the confidence interval.\n\nThese two sets are connected by the tautology\n\n$$\n\\bar{x} \\in A(\\theta) \\Longleftrightarrow \\theta \\in C(x_1, \\dots, x_n). \n$$\n\nBasically,\n\n* If a sample $x_1, \\dots, x_n$ is in $A(\\theta)$, then the CI computed with that sample includes $\\theta$.\n​\n* Conversely, if $\\theta$ is in the CI computed from the sample $x_1, \\dots, x_n$, then that sample is in $A(\\theta)$.\n\n\nThe relationship between hypothesis tests' acceptance regions and confidence intervals demonstrates why the values in the confidence interval are not statistically significantly different than the point estimate. The sample we observed is contained in the acceptance region of all $\\theta$'s present in the confidence interval.\n\nThis relationship also shows that hypothesis tests and confidence intervals provide consistent interpretations of the data, but from different angles. @casella2002statistical sum it up nicely:\n\n> The hypothesis test fixes the parameter and asks what sample values (the acceptance region) are consistent with that fixed value. The confidence set fixes the sample value and asks what parameter values (the confidence interval) make this sample value most plausible. (p. 421)\n\n\n# Conclusion\n\nThe main thing I think you should take away from this post is that a perfectly valid way to interpret a confidence interval is that it's an interval that contains values for the population parameter that are consistent with the observed data. Personally, I think that this interpretation is also more useful than the \"usual\" one because it permits a clear interpetation for the confidence interval in front of you.\n\n  \n<!-- This interval is a random quantity (it's a function of our random sample), and has the property that\n$$\nP\\left(\\bar{x} - \\frac{s}{\\sqrt{n}}t_{(1-\\alpha/2), n-1} < \\mu <  \\bar{x} + t_{(1-\\alpha/2), n-1} \\frac{s}{\\sqrt{n}}\\right) = 1 - \\alpha\n$$\n\nNote that the *interval* is random and the true parameter value $\\mu$ is fixed. Once we've constructed an interval from our sample, the interval either contains $\\mu$ or it does not. So the above statement says that if we *sample* a confidence interval from the distribution of possible confidence intervals, there is a $1-\\alpha$ probability that it will cover the paramter value $\\mu$. -->\n\n\n<!-- > It is tempting to say (and many experimenters do) that “the probability is 90% that λ is in the interval [.262, 1.184].” Within classical statistics, however, such a statement is invalid since the parameter is assumed fixed. Formally, the interval [.262, 1.184] is one of the possible realized values of the random interval [1 2n χ2 2Y,.95, 1 2n χ2 2(Y +1),.05] and, since the parameter λ does not move, λ is in the realized interval [.262, 1.184] with probability either 0 or 1. When we say that the realized interval [.262, 1.184] has a 90% chance of coverage, we only mean that we know that 90% of the sample points of the random interval cover the true parameter. \n\nProblems:\n\n- there is only *a priori* a 95% chance that the interval contains the true value \n- NOT a 95% chance the true parameter value is in this range\n- interpretation requires reference to a series of infinity hypothetical experiments -->\n  \n\n<!-- \n\n___\n\n# Intuition\n\nWhy is it that we fail to reject any $\\theta$ in the confidence interval?\n\n1. look at approximate sampling distribution of $\\bar{x}$\n2. suppose it's our \"null\" distribution\n3. at alpha = 0.05, we only reject values outside of the 0.05 region.\n4. by definition, the regions > 5% coincide with the confidence intervals...\n$$\n\\frac{\\bar{x} - \\mu}{s / \\sqrt{n}} \\sim \\text{Student-}t\\left(n-1, 0, 1\\right)\n$$\n\n$$\n\\Longrightarrow\n$$\n\n$$\n\\bar{x} \\sim \\text{Student-}t\\left(n - 1, \\mu, \\frac{s}{\\sqrt{n}}\\right)\n$$\n$$\n\\Longrightarrow\n$$\n\n$$\n\\bar{x} \\sim \\text{Student-}t\\left(n - 1, \\bar{x}, \\frac{s}{\\sqrt{n}}\\right)\n$$\n\nWe are saying that we can approximate the sampling distribution of $\\bar{x}$ as $\\bar{x} \\sim \\text{Student-}t\\left(n - 1, \\bar{x}, \\frac{s}{\\sqrt{n}}\\right)$.\n\nLet's take the 0.025 and 0.975 quantiles of this distribution, i.e., the bounds capturing the middle 95% of the distribution. We can use the `extraDistr` package find the quantiles of the [location-scale formulation of the $\\text{Student-}t$ distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution#Location-scale_t_distribution).\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nx_bar <- mean(x)\nstd_err_x_bar <- sd(x) / sqrt(n)\n\nalpha <- 0.05\nt_crit_lb <- extraDistr::qlst(alpha / 2, df = n - 1, x_bar, std_err_x_bar)\nt_crit_ub <- extraDistr::qlst(1 - (alpha / 2), df = n - 1, x_bar, std_err_x_bar)\n\nc(t_crit_lb, t_crit_ub)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 49.43518 56.77577\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nci\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      lb       ub \n49.43518 56.77577 \n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf <- data.frame(\n  x = seq(\n    from = x_bar - 4 * std_err_x_bar,\n    to   = x_bar + 4 * std_err_x_bar,\n    length.out = 100\n  )\n)\n\n\ndf$y <- extraDistr::dlst(df$x, df = n - 1, x_bar, std_err_x_bar)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_line() +\n  geom_ribbon(\n    data = subset(df, x >= t_crit_lb & x <= t_crit_ub),\n    aes(ymin = 0, ymax = y),\n    fill = \"lightblue\",\n    alpha = 0.5\n  ) + \n  geom_errorbarh(aes(xmin = ci[1], xmax = ci[2], y = 0), height = 0.1) +\n  theme_minimal() +\n  labs(\n    title = \"Approximate sampling distribution of the mean\",\n    x = \"x\",\n    y = \"Density\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=576}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(c(t_crit_lb, t_crit_ub))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 49.43518 56.77577\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(ci)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      lb       ub \n49.43518 56.77577 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n--- -->\n\n\n\n\n# References\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}