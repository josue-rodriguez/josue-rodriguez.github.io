{
  "hash": "52e71bc2764403ed3839404bdd39ba33",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Connecting Scalar and Matrix Notation of OLS\nauthor: Josue Rodriguez\ndate: \"2024-10-12\"\nformat:\n  html:\n    fig-align: center\n    include-before-body: ../../resources/mathjax.tex\n    toc: true\nfrom: markdown+emoji\ncategories: [statistics, regression]\n---\n\n\nI don't know. Maybe my journey in statistics has been nontraditional in that much of my statistical training came from social-behavioral stats classes, where the math-y bits of statistics are de-emphasized, and so I missed out on a lot of the intuition and proofs that many statistics students see in their undergraduate courses.\n\nFor me, this is certainly the case in connecting the estimating equations for the regression parameters (and their variances) that you learn in intro statistics courses to the estimating equation in the matrix form of OLS. This post is an attempt at making that connection clearer for myself. \n\nThe derivations below largely come from Chapter 11 in Cosma Shalizi's *The Truth about Linear Regression*. Where I could, I filled in some details.\n\n# OLS in scalar notation\n\nOLS regression is typically presented in its simplest, clearest form in introductory statistics classes\n\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i,\\quad i = 1, \\dots, n\n$$\n\n\nThe goal is to predict $y_i$ from $x_i$, and the optimal linear prediction of $y_i$ given $x_i$ is obtained when the regression parameters $\\beta_0$ and $\\beta_1$ chosen are the ones that minimize the sum of the squared residuals (RSS). The parameters accomplishing this are given by the following equations\n\n$$\n\\begin{align}\n\\hat \\beta_1 &= \\frac{\\hat{\\text{Cov}[x, y]}}{\\hat \\sigma^2_x}  \\\\\n\\hat \\beta_0 &= \\bar y -  \\hat \\beta_1 \\bar x.\n\\end{align}\n$$\n\nAssuming homoskedastic and independent errors, the variances of the regression parameters can be calculated as follows\n\n$$\n\\text{Var}(\\beta_0) = \\frac{\\sigma^2_\\epsilon}{n} \\left( 1 + \\frac{\\bar x^2}{\\sigma^2_x} \\right).\n$$\n\n$$\n\\text{Var}(\\beta_1) = \\frac{\\sigma^2_{\\epsilon}}{\\sum_{i=1}^n (x_i - \\bar x)^2}\n$$\n\n\n\n# OLS matrix form\n\nWhile the equations in scalar notation are intuitive for simple regression, they become unwieldy with multiple predictors. This is where matrix notation comes in handy since it gives us a nice, compact notation. However, we'll stick to the case of a single predictor so that we can focus on connecting the scalar form to the matrix form.\n\nThe scalar equations and be combined and re-written using vectors and matrices\n$$\n\\begin{align}\n\\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n \n\\end{bmatrix} &= \n\\begin{bmatrix}\n\\beta_0 + \\beta_1 x_i + \\epsilon_1\\\\\n\\vdots \\\\\n\\beta_0 + \\beta_1 x_n + \\epsilon_n\n\\end{bmatrix} \\\\\n%------------------\n&= \n\\begin{bmatrix}\n1 & x_1 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{bmatrix}\n+ \n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\vdots \\\\\n\\epsilon_i \\\\\n\\end{bmatrix}.\n\\end{align}\n$$\n\n\nThese can be compactly written as \n$$\n\\begin{align}\n\\mathbf{y} = \\mathbf{x} \\bm{\\beta} + \\bm{\\epsilon}.\n\\end{align}\n$$\n\nIn matrix notation, the regression parameters that minimize the RSS are given by \n$$\n\\hat{\\bm{\\beta}} = \\begin{bmatrix} \\hat\\beta_0 \\\\ \\hat\\beta_1\\end{bmatrix} = \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1} \\mathbf{x}'\\mathbf{y}\n$$\n\nand the estimated variance-covariance matrix of the coefficients by\n\n$$\n\\bm{\\Sigma} = \\begin{bmatrix} \\text{Var}[\\hat\\beta_0] & \\cdot \\\\ \\cdot &  \\text{Var}[\\hat\\beta_1]\\end{bmatrix} = \\sigma^2_\\epsilon (\\bm{x}'\\bm{x})^{-1}.\n$$\n\n\n# Deriving the matrix OLS estimator\n\nThe first step we'll take in connecting the scalar equations to the matrix equation is to show that the matrix estimator indeed yields a parameter vector that minimizes the RSS. \n\nIn matrix form, the residuals can be expressed as \n$$\n\\begin{align}\n\\bm \\epsilon &= \\begin{bmatrix}\ny_1 - \\hat y_1 \\\\\n\\vdots \\\\\ny_n - \\hat y_n\n\\end{bmatrix} \\\\\n&= \\mathbf{y} - \\hat{\\mathbf{y}} \\\\\n&= \\mathbf{y} - \\mathbf{x} \\bm{\\beta} \\\\\n\\end{align}\n$$\n\nand then we write also write the residual sum of squares in matrix form residual sum of squares can then be expressed as\n\n$$\n\\begin{align}\n\\text{RSS} &= \\epsilon'\\epsilon \\\\\n&= \\left(\\mathbf{y} - \\mathbf{x} \\bm{\\beta} \\right)' \\left(\\mathbf{y} - \\mathbf{x} \\bm{\\beta} \\right).\n\\end{align}\n$$\n\nExpanding that equation yields\n\n$$\n=\n  \\mathbf{y}'\\mathbf{y} -\n  2\\bm{\\beta}'\\mathbf{x}'\\mathbf{y} +\n  \\bm{\\beta}'\\mathbf{x}'\\mathbf{x}\\bm{\\beta}.\n$$\n\nIf you're curious, the intervening steps can be found by expanding the box below.\n\n::: {.callout-note title=\"RSS Details\" collapse=true icon=false}\nThere exists a rule for the tranpose of a product that says $(AB)' = B'A'$. Applying that rule, we get\n\n$$\n\\text{RSS} = \\left(\\mathbf{y}' -\\bm{\\beta}'\\mathbf{x}'\\right) \\left(\\mathbf{y} - \\mathbf{x} \\bm{\\beta} \\right)\n$$\n\nExpanding the equation we get\n  \n$$\n= \\mathbf{y}'\\mathbf{y} -\n\\mathbf{y}'\\mathbf{x}\\bm{\\beta} - \n\\bm{\\beta}'\\mathbf{x}'\\mathbf{y} +\n\\bm{\\beta}'\\mathbf{x}'\\mathbf{x}\\bm{\\beta}\n$$\n\nNow, notice this little bit here $\\bm{\\beta}'\\mathbf{x}'\\mathbf{y}$. If we again apply the above tranpose of a product rule we get $\\bm{\\beta}'\\mathbf{x}'\\mathbf{y} = \\left(\\mathbf{y}'\\mathbf{x}\\bm{\\beta} \\right)'$. \n\n* Also, this results in a $1 \\times 1$ matrix, so $\\left(\\mathbf{y}'\\mathbf{x}\\bm{\\beta} \\right)' = \\mathbf{y}'\\mathbf{x}\\bm{\\beta}$.\n\nSubstituting $\\mathbf{y}'\\mathbf{x}\\bm{\\beta}$ in the RSS equation with $\\bm{\\beta}'\\mathbf{x}'\\mathbf{y}$ we get\n\n\n$$\n\\begin{align}\n&= \\mathbf{y}'\\mathbf{y} -\n\\bm{\\beta}'\\mathbf{x}'\\mathbf{y} - \n\\bm{\\beta}'\\mathbf{x}'\\mathbf{y} ++\n\\bm{\\beta}'\\mathbf{x}'\\mathbf{x}\\bm{\\beta} \\\\\n&= \n  \\mathbf{y}'\\mathbf{y} -\n  2\\bm{\\beta}'\\mathbf{x}'\\mathbf{y} +\n  \\bm{\\beta}'\\mathbf{x}'\\mathbf{x}\\bm{\\beta}\n\\end{align}\n$$\n:::\n\n---\n\nNow, we want to find the vector $\\bm{\\beta}$ that *minimizes* the RSS. So we must now find the gradient of the RSS with respect to $\\bm{\\beta}$, set it to zero, and solve for $\\bm{\\beta}$.\n\nThe gradient can be worked out to be\n\n$$\n\\begin{align}\n\\nabla \\text{RSS} &= \n  \\nabla \\mathbf{y}'\\mathbf{y} -\n  2 \\nabla \\bm{\\beta}'\\mathbf{x}'\\mathbf{y} +\n  \\nabla \\bm{\\beta}'\\mathbf{x}'\\mathbf{x}\\bm{\\beta}\n\\\\\n&= 2 \\left( \\mathbf{x}'\\mathbf{x}\\bm{\\beta} - \\mathbf{x}'\\mathbf{y} \\right).\n\\end{align}\n$$\n\nAgain, the curious can find details by expanding the box below. \n\n::: {.callout-note title=\"Gradient Details\" collapse=true icon=false}\nThe gradient can be worked out term by term.\n\n$$\n\\nabla \\mathbf{y}'\\mathbf{y} = 0\n$$\n\n* Because $\\mathbf{y}'\\mathbf{y}$ is fixed.\n\n$$\n-2 \\nabla \\bm{\\beta}'\\mathbf{x}'\\mathbf{y} = -2 \\mathbf{x}' \\mathbf{y}\n$$\n\n* Because $\\frac{\\partial \\mathbf{x}'\\mathbf{a}}{\\partial \\mathbf{x}} = \\mathbf{a}$\n\n$$\n\\nabla \\bm{\\beta}'\\mathbf{x}'\\mathbf{x}\\bm{\\beta} = 2 \\mathbf{x}'\\mathbf{x}\\bm{\\beta}\n$$\n\n* Because $\\mathbf{x}'\\mathbf{x}$ is symmetric, and if $\\mathbf{A}$ is symmetric, $\\frac{\\partial \\mathbf{x}'\\mathbf{A}\\mathbf{x}}{\\partial \\mathbf{x}} = 2\\mathbf{A}\\mathbf{x}$\n\nGluing it back together we get\n\n$$\n\\begin{align}\n\\nabla \\text{RSS} &= \n  -2 \\mathbf{x}' \\mathbf{y} + 2 \\mathbf{x}'\\mathbf{x}\\bm{\\beta} \n  \\\\\n&= 2 \\left(\n  \\mathbf{x}'\\mathbf{x}\\bm{\\beta} - \\mathbf{x}' \\mathbf{y}\n\\right) \n\\end{align}\n$$\n:::\n\nNow, remember that $\\mathbf{x}$ and $\\mathbf{y}$ are sample data. We can *estimate* the coefficient vectory by setting the gradient to zero and solving for it\n\n$$\n\\begin{align}\n2 \\left(\n  \\mathbf{x}'\\mathbf{x}\\hat{\\bm{\\beta}} - \\mathbf{x}' \\mathbf{y}\n  \\right) \n  &= 0 \\\\\n\\mathbf{x}'\\mathbf{x}\\hat{\\bm{\\beta}} - \\mathbf{x}' \\mathbf{y} &= 0 \\\\\n\\mathbf{x}'\\mathbf{x}\\hat{\\bm{\\beta}} &= \\mathbf{x}' \\mathbf{y} \\\\\n\\hat{\\bm{\\beta}} &= \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}' \\mathbf{y}.\n\\end{align}\n$$\n\nTo see that this $\\hat{\\bm{\\beta}}$ is a *minimum*, we can do a second derivative test. We get the Hessian by taking the gradient and differentiating again with respect to $\\bm{\\beta}$: $2 \\mathbf{x}'\\mathbf{x}$. If $\\mathbf{x}$ is full rank and positive semi-definite then so is its Gram matrix  $\\mathbf{x}'\\mathbf{x}$, and the vector $\\bm\\beta$ is a minimum. \n\nExcellent. We see that the estimating equation $\\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}' \\mathbf{y}$ indeed results in a vector of parameter estimates that minimizes the RSS.\n\n# Parameter estimates\n\nOur next step is to make sure that the elements in $\\hat{\\bm{\\beta}}$ correspond to their scalar counterparts.\n\nThat is, we want to confirm that \n\n$$\n\\hat{\\bm{\\beta}} = \n\\begin{bmatrix}\\hat\\beta_0 \\\\ \\hat\\beta_1\\end{bmatrix} = \n\\begin{bmatrix}\n\\bar y -  \\hat \\beta_1 \\bar x \\\\\n\\frac{{\\text{Cov}[x, y]}}{\\hat \\sigma^2_x} \n\\end{bmatrix}.\n$$\n\nAs a starting point, we'll introduce a normalizing factor $n^{-1}$ to the estimating equation[^1] so that \n\n[^1]: This will be handy in letting us define the elements in the relevant vectors and matrices as means and variances.\n\n$$\n\\hat{\\bm{\\beta}} =\n\\left(\n   n^{-1}\\mathbf{x}'\\mathbf{x}\n\\right)^{-1} n^{-1}\\mathbf{x}'\\mathbf{y}.\n$$\n\n---\n\n\nWe'll then compute this product in parts, beginning with the terms inside the parentheses. I'll note here that in what follows, estimates of variances are defined as their maximum likelihood estimates.\n\n$$\n\\begin{align}\nn^{-1}\\mathbf{x}'\\mathbf{x} &= \n\\frac1n\n\\begin{bmatrix}\n1 & \\dots & 1 \\\\\nx_1 & \\dots & x_n\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & x_1 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n\n\\end{bmatrix} \\\\\n% ---------\n&= \\frac1n \\begin{bmatrix}\nn & \\sum x_i \\\\\n\\sum x_i & \\sum x^2_i\n\\end{bmatrix} \\\\\n% ---------\n&= \\begin{bmatrix}\n1 & \\bar{x}\\\\\n\\bar{x}& \\bar{x^2}\n\\end{bmatrix}\n\\end{align}\n$$\n\nLet's not forget that we need to find the inverse of the resulting matrix [^2]\n\n[^2]: The inverse of a $2 \\times 2$ matrix is given by $\\left[\\begin{smallmatrix}a & b \\\\ c & d\\end{smallmatrix}\\right]^{-1} = \\frac{1}{ad-bc}\\left[\\begin{smallmatrix}d & -b \\\\ -c & a\\end{smallmatrix}\\right]$.\n\n$$\n\\begin{align}\n\\left(n^{-1}\\mathbf{x}'\\mathbf{x}\\right)^{-1} &=\n  \\begin{bmatrix}\n    1 & \\sum \\bar{x}\\\\\n    \\bar{x}& \\sum \\bar{x^2}\n  \\end{bmatrix}^{-1} \\\\\n% ---------\n&= \\frac{1}{\\bar{x^2} - \\bar{x}^2} \\begin{bmatrix}\n\\bar{x^2} & -\\bar{x}\\\\\n-\\bar{x} & 1  \n\\end{bmatrix}\\\\\n% ---------\n&= \\frac{1}{\\hat\\sigma^2_x} \\begin{bmatrix}\n\\bar{x^2} & -\\bar{x}\\\\\n-\\bar{x} & 1  \n\\end{bmatrix}.\n\\end{align}\n$$\n\n* We get $\\hat\\sigma^2_x$ because $\\bar{x^2} - \\bar{x}^2$ estimates $\\text{Var}[x] = \\mathbb{E}[x^2] - \\mathbb{E}[x]^2$.\n\n___\n\nNext, we'll work through the term outside the parentheses \n\n$$\n\\begin{align}\n n^{-1}\\mathbf{x}'\\mathbf{y} &=\n\\frac1n \\begin{bmatrix}\n  1 & \\dots & 1 \\\\\n  x_1 & \\dots & x_n\n\\end{bmatrix}\n\\begin{bmatrix}\n  y_i \\\\\n  \\vdots \\\\\n  y_n\n\\end{bmatrix}\\\\\n% ---------\n&= \\frac1n \\begin{bmatrix}\n  \\sum y_i \\\\\n  \\sum x_i y_i\n\\end{bmatrix} \\\\\n% ---------\n&= \\begin{bmatrix}\n\\bar{y} \\\\\n\\overline{xy}\n\\end{bmatrix}\n\\end{align}\n$$\n\n___\n\nStitching it all together we see that\n\n\n$$\n\\begin{align}\n\\hat{\\bm{\\beta}} &=\n\\left(\n  n^{-1}\\mathbf{x}'\\mathbf{x}\n\\right)^{-1} n^{-1}\\mathbf{x}'\\mathbf{y} \\\\\n% ---------\n&= \\frac{1}{\\hat\\sigma^2_x} \n\\begin{bmatrix}\n  \\bar{x^2} & -\\bar{x}\\\\\n  -\\bar{x} & 1  \n\\end{bmatrix}\n\\begin{bmatrix}\n  \\bar{y} \\\\\n  \\overline{xy}\n\\end{bmatrix} \\\\\n% ---------\n&= \\frac{1}{\\hat\\sigma^2_x}\n\\begin{bmatrix}\n  \\bar{x^2}\\bar{y} - \\bar{x} \\overline{xy} \\\\\n  - \\bar{x} \\bar{y} + \\overline{xy}\n\\end{bmatrix}.\n% ---------\n\\end{align}\n$$\n\nAt this point, it still feels unclear how the elements of this vector correspond to scalar equations for $\\hat\\beta_0$ and $\\hat\\beta_1$, even though we have seen that it's elements minimize the RSS. To continue, we'll have re-express the elements of the vector by taking advantage of the following definitions\n\n* $\\hat{\\sigma^2_x} = \\bar{x^2} - \\bar{x}^2$\n* $\\hat{\\text{Cov}[x, y]} = \\overline{xy} - \\bar{x}\\bar{y}$\n\n\nDoing so let's us re-write the vector as follows\n$$\n\\begin{align}\n&= \\frac{1}{\\hat\\sigma^2_x}\n\\begin{bmatrix}\n  \\left(\\hat\\sigma^2_x + \\bar{x}^2\\right)\\bar{y} - \\bar{x}\\left(\\hat{\\text{Cov}[x,y]} + \\bar{x} \\bar{y}\\right) \\\\\n  \\hat{\\text{Cov}[x,y]} \n\\end{bmatrix} \\\\\n% ---------\n&= \\frac{1}{\\hat\\sigma^2_x}\n\\begin{bmatrix}\n  \\hat\\sigma^2_x\\bar{y} + \\bar{x}^2\\bar{y} - \\bar{x}\\hat{\\text{Cov}[x,y]} - \\bar{x}^2 \\bar{y} \\\\\n  \\hat{\\text{Cov}[x,y]} \n\\end{bmatrix}.\n% ---------\n\\end{align}\n$$\n\nOut last step here is distributing $\\frac{1}{\\hat\\sigma^2_x}$\n\n$$\n\\begin{align}\n&= \n\\begin{bmatrix}\n  \\bar{y} - \\frac{\\hat{\\text{Cov}[x,y]}}{\\hat\\sigma^2_x}\\bar{x} \\\\\n  \\frac{\\hat{\\text{Cov}[x,y]}}{\\hat\\sigma^2_x}\n\\end{bmatrix} \\\\\n% ---------\n&= \n\\begin{bmatrix}\n  \\bar{y} - \\hat\\beta_1\\bar{x} \\\\\n  \\hat \\beta_1\n\\end{bmatrix}.\n\\end{align}\n$$\n\nAnd there it is. Although it was a bit tedious, we can see that the elements in $\\hat{\\bm{\\beta}}$ indeed correspond to the scalar equations for $\\hat\\beta_0$ and $\\hat\\beta_1$. \n\n# Deriving the sampling distribution of $\\hat{\\boldsymbol{\\beta}}$\n\nTo perform hypothesis tests and compute confidence intervals for the elements in $\\hat{\\bm{\\beta}}$, we need to understand its sampling distribution.  We can do this by splitting $\\hat{\\bm{\\beta}}$ into a deterministic part and a random part.\n\nNote that up to this point, we haven't made any distributional assumptions. Now, though, we'll assume that usual assumption that the residuals are distributed $\\bm{\\epsilon} \\sim \\mathcal{N}\\left(\\mathbf{0}, \\sigma^2_{\\epsilon}\\mathbf{I}\\right)$.\n\n\nRemember that we've defined $\\mathbf{y} = \\mathbf{x}\\bm{\\beta} + \\bm{\\epsilon}$ so\n\n$$\n\\begin{align}\n  \\hat{\\bm{\\beta}} &= \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}' \\mathbf{y} \\\\\n  % ---------\n  &= \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}' \\left(\\mathbf{x}\\bm{\\beta} + \\bm{\\epsilon}\\right) \\\\\n  % ---------\n  &= \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}'\\mathbf{x}\\bm{\\beta} +  \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}'\\bm{\\epsilon} \\\\\n  % ---------\n  &= \\bm{\\beta} + \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}'\\bm{\\epsilon}.\n\\end{align}\n$$\n\nTaking the expectation we get\n\n$$\n\\begin{align}\n\\mathbb{E}[\\hat{\\bm{\\beta}}] &= \\mathbb{E}[\\bm{\\beta} + \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}'\\bm{\\epsilon}] \\\\\n% ---------\n &= \\mathbb{E}[\\bm{\\beta}] + \\mathbb{E}[\\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}'\\bm{\\epsilon}] \\\\\n % ---------\n &= \\bm{\\beta} + \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}'\\mathbb{E}[\\bm{\\epsilon}] \\\\\n\\end{align}\n$$\n\nand $\\mathbb{E}[\\bm{\\epsilon}] = 0$, we're left with \n\n$$\n\\mathbb{E}[\\hat{\\bm{\\beta}}] = \\bm{\\beta}.\n$$\n\nTurning our attention to the variance-covariance matrix of the coefficients\n\n$$\n\\begin{align}\n\\text{Var}[\\hat{\\bm{\\beta}}] &= \\text{Var}[\\bm{\\beta} + \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}'\\bm{\\epsilon}] \\\\\n% ---------\n&= \\text{Var}[\\bm{\\beta}] + \\text{Var}[\\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}'\\bm{\\epsilon}]\n\\end{align}\n$$\n\nThe variance of $\\bm{\\beta}$ is 0 because it is a fixed vector. To work out the variance for the remaining term we'll use the fact that if $\\mathbf{a}$ is fixed and $\\mathbf{X}$ is random, then $\\text{Var}[\\mathbf{a}\\mathbf{X}] = \\mathbf{a} \\text{Var}[\\mathbf{X}]\\mathbf{a}'$\n\n$$\n\\begin{align}\n&= \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}' \\text{Var}[\\bm{\\epsilon}] \\left( \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}'\\right)' \\\\\n% ---------\n&= \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}' \\sigma^2_{\\epsilon}\\mathbf{I} \\mathbf{x}\\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1} \\\\\n% --------- Multiplying by I does't change the matrix\n&= \\sigma^2_{\\epsilon}  \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}'\\mathbf{x}\\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1} \\\\\n% --------- \n&= \\sigma^2_{\\epsilon}  \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\n\\end{align}.\n$$\n  \n\nGreat, we can now see that the estimated regression coefficients are normally distributed with mean vector $\\beta$[^3] (the true regression coefficients), and variance-covariance matrix $\\bm{\\Sigma} =  \\sigma^2_{\\epsilon}  \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}$. \n\n[^3]: This stems from the fact that we defined $\\hat{\\bm{\\beta}} = \\bm{\\beta} + \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}'\\bm{\\epsilon}$ and the normality of $\\bm{\\epsilon}$ is preserved under affine transformations.\n\n$$\n\\hat{\\bm\\beta} \\sim \\mathcal{N} \\left(\\bm{\\beta}, \\bm{\\Sigma}\\right).\n$$\n\nIn practice, though, we don't know $\\sigma^2_{\\epsilon}$ so we instead use $\\hat{\\bm{\\Sigma}} = \\hat{\\sigma^2_\\epsilon}\\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}$.\n\n\n## Elements of the variance-covariance matrix\n\nAs we did with the parameter estimates, we want to make sure that our matrix representation of the parameter variances correspond to their scalar counterparts. \n\nTo find out, we'll again start by adding in a normalizing term of $n^{-1}$ so that\n\n$$\n\\begin{align}\n{\\bm{\\Sigma}} &= n^{-1}\\sigma^2_{\\epsilon}  \\left(n^{-1}\\mathbf{x}'\\mathbf{x}\\right)^{-1} \\\\\n% --------- \n&= \\frac{\\sigma^2_{\\epsilon}}{n}  \\left(\\frac{1}{\\hat\\sigma^2_x} \\begin{bmatrix}\n\\bar{x^2} & -\\bar{x}\\\\\n-\\bar{x} & 1  \n\\end{bmatrix}\n\\right)\n\\end{align}\n$$\n\n\nThe variance for $\\hat\\beta_0$ corresponds to the first diagonal element of ${\\bm{\\Sigma}}$. Multiplying the relevant terms gets us\n\n$$\n\\begin{align}\n\\text{Var}[\\hat\\beta_0] &= \\frac{\\sigma^2_{\\epsilon}}{n} \\cdot \\frac{1}{\\hat\\sigma^2_x} \\cdot \\bar{x^2} \\\\\n% --------- \n&= \\frac{\\sigma^2_{\\epsilon}\\bar{x^2} }{n \\hat\\sigma^2_x} \\\\\n% --------- \n&= \\frac{\\sigma^2_{\\epsilon}( \\bar{x}^2  + \\hat\\sigma^2_x )}{n \\hat\\sigma^2_x} \\\\\n% --------- \n&= \\frac{\\sigma^2_{\\epsilon}\\bar{x}^2  + \\sigma^2_{\\epsilon}\\hat\\sigma^2_x}{n \\hat\\sigma^2_x} \\\\\n% --------- \n&= \\frac{\\sigma^2_{\\epsilon}\\bar{x}^2}{n \\hat\\sigma^2_x} + \\frac{\\sigma^2_{\\epsilon}\\hat\\sigma^2_x}{n \\hat\\sigma^2_x} \\\\\n% --------- \n&= \\frac{\\sigma^2_{\\epsilon}}{n} \\left(\n  \\frac{\\bar{x}^2}{\\sigma^2_x} + 1\n\\right).\n\\end{align}\n$$\n\nThe variance for $\\hat\\beta_1$ corresponds to the second diagonal element of ${\\bm{\\Sigma}}$, and again multiplying the relevant terms\n\n$$\n\\begin{align}\n\\text{Var}[\\hat\\beta_1] &= \\frac{\\sigma^2_{\\epsilon}}{n} \\cdot \\frac{1}{\\hat\\sigma^2_x} \\cdot 1 \\\\\n% --------- \n&= \\frac{\\sigma^2_{\\epsilon}}{n} \\cdot \\frac{1}{\\frac{\\sum (x_i - \\bar{x})^2}{n}}\\\\\n% --------- \n&= \\frac{\\sigma^2_{\\epsilon}}{\\frac{n\\sum (x_i - \\bar{x})^2}{n}}\\\\\n% --------- \n&= \\frac{\\sigma^2_{\\epsilon}}{\\sum_{i=1}^n\\left(x_i - \\bar{x}\\right)^2}.\n\\end{align}\n$$\n\nNice! With that, we connected the variances for the regression parameters obtained via the matrix form of OLS to those obtained through scalar notation. \n\n# Conclusion\n\nBy carefully working through the matrix derivations, we've demonstrated that the OLS parameter estimates and their variances derived in matrix form are equivalent to those obtained from the traditional scalar equations. This exercise not only bridges the gap between the two approaches but also showcases the elegance and efficiency of matrix algebra in statistical modeling.\n\nFor those like me, who initially learned statistics in a less mathematically rigorous context, revisiting these foundational concepts through the lens of linear algebra can deepen understanding and appreciation for the underlying mechanics of regression analysis.\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}