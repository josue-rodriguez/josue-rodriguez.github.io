<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Josue Rodriguez</title>
    <link>https://josue.rbind.io/post/</link>
      <atom:link href="https://josue.rbind.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 18 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://josue.rbind.io/img/icon.png</url>
      <title>Posts</title>
      <link>https://josue.rbind.io/post/</link>
    </image>
    
    <item>
      <title>Model Selection Bias</title>
      <link>https://josue.rbind.io/post/model-selection-bias/</link>
      <pubDate>Thu, 18 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://josue.rbind.io/post/model-selection-bias/</guid>
      <description>


&lt;p&gt;Over the last few months, a frequent topic of conversation with my &lt;a href=&#34;https://twitter.com/wdonald_1985&#34;&gt;lab mate Donny&lt;/a&gt; has been the issue of &lt;em&gt;valid&lt;/em&gt; inference following model selection, or model selection bias. This problem has been recognized since &lt;a href=&#34;https://mathscinet.ams.org/mathscinet-getitem?mr=0150864&#34;&gt;at least 1963&lt;/a&gt; and has been written about extensively since then. Some resources I have found both helpful and accessible in understanding model selection bias can be found &lt;a href=&#34;https://arxiv.org/abs/1306.1059&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://www.jstor.org/stable/3533623&#34;&gt;here&lt;/a&gt;, and &lt;a href=&#34;http://bactra.org/notebooks/post-model-selection-inference.html&#34;&gt;here&lt;/a&gt;. However, this issue is still pervasive among social and behavioral scientists,&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; so I am writing a short post here in hopes of clarifying the ramifications of drawing inference after selecting a model.&lt;/p&gt;
&lt;div id=&#34;what-is-model-selection-bias&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is model selection bias?&lt;/h1&gt;
&lt;p&gt;To understand model selection bias, it is helpful to understand what constitutes model selection.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model selection&lt;/strong&gt; occurs when a data-driven procedure is used to select variables, or the final model. This includes when statistical models are chosen based on things like minimizing cross-validation error, penalty-based criteria (e.g., AIC, LASSO, etc.) or stagewise selection, among others.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model selection bias&lt;/strong&gt; occurs when, after a model selection procedure, researchers proceed with inference “as usual” (i.e., as if the model were known &lt;em&gt;a priori&lt;/em&gt;; &lt;span class=&#34;citation&#34;&gt;(&lt;span class=&#34;citeproc-not-found&#34; data-reference-id=&#34;leebMODEL2005&#34;&gt;&lt;strong&gt;???&lt;/strong&gt;&lt;/span&gt;)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The notion of model selection bias is nicely summarized in &lt;span class=&#34;citation&#34;&gt;Berk et al. (&lt;a href=&#34;#ref-berkValid2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Classical statistical theory grants validity of statistical tests and confidence
intervals assuming a wall of separation between the selection of a model and the
analysis of the data being modeled. In practice, this separation rarely exists, and
more often a model is “found” by a data-driven selection process. As a consequence
inferential guarantees derived from classical theory are invalidated. Among
model selection methods that are problematic for classical inference, variable selection stands out because it is regularly taught, commonly practiced and highly
researched as a technology. (p. 802)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In psychology, perhaps the most common example of model selection bias occurs with linear models. Say you estimate a multiple regression and after examining the predictors and their associated &lt;em&gt;p&lt;/em&gt;-values, you decide to drop those with “non-significant” &lt;em&gt;p&lt;/em&gt;-values according to some &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; level. You then refit the model using only the “significant” predictors and proceed with inference. This procedure is a form of model selection and the resulting inference will suffer from model selection bias.&lt;/p&gt;
&lt;p&gt;Another common example occurs with regularization, such as using lasso or ridge penalties. These methods are often used due to the thought that they guard against spurious relations and overfitting. However, these forms of regularization bias estimates toward zero, and this presents issues for computing &lt;em&gt;valid&lt;/em&gt; &lt;em&gt;p&lt;/em&gt;-values and confidence intervals.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-does-model-selection-bias-matter&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why does model selection bias matter?&lt;/h1&gt;
&lt;p&gt;Although students are commonly taught to remove “insignificant” predictors or use information criteria to find the most parsimonious model, &lt;strong&gt;model selection bias distorts the sampling distribution of parameter estimates and results in more Type I errors&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;span class=&#34;citeproc-not-found&#34; data-reference-id=&#34;leebMODEL2005&#34;&gt;&lt;strong&gt;???&lt;/strong&gt;&lt;/span&gt;; Berk et al., &lt;a href=&#34;#ref-berkValid2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;.
&lt;!-- here add the note that to be selected, the effect must be large, which suggests that the sampling distribution is not, say, normal, but truncated normal (see for example X).. --&gt;&lt;/p&gt;
&lt;div id=&#34;why-does-this-occur&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why does this occur?&lt;/h2&gt;
&lt;p&gt;An assumption behind the majority of inferential procedures is that the model is &lt;strong&gt;fixed&lt;/strong&gt;, but model selection makes the model itself &lt;strong&gt;random&lt;/strong&gt;. Inferential procedures typically do not account for this stochastic aspect. Using confidence intervals, let’s take a second to think about why model selection introduces randomness.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Confidence_interval#Meaning_and_interpretation&#34;&gt;Wikipedia&lt;/a&gt; defines a 90% confidence interval in terms of sampling by stating&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Were this procedure to be repeated on numerous samples, the fraction of calculated confidence intervals (which would differ for each sample) that encompass the true population parameter would tend toward 90%.&#34;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What I would like to emphasize here is that that valid confidence intervals have a long run guarantee of covering the true population parameter (approximately) 90 out of 100 times, and this guarantee is based on carrying out the same procedure repeatedly. In this context, procedure refers to an &lt;em&gt;estimation&lt;/em&gt; procedure and implies that the statistical model used for inference does not change from sample to sample.&lt;/p&gt;
&lt;p&gt;Now, suppose we have collected some data and we have some candidate set of predictors. We then pick a subset of these predictors according to some model selection procedure and estimate a model with this subset. The end result is an estimation procedure that is &lt;em&gt;conditional on the selected variables&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;If we do this repeatedly — collect some new data, select a model, and fit it with the selected predictors — there is no guarantee that the estimation procedure will have the same predictors each time.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; It is this randomness introduced by model selection that invalidates the properties of classical inference.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;small-simulation-study&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Small simulation study&lt;/h1&gt;
&lt;p&gt;I would like to illustrate the effect of model selection on inference with a recent project I’ve been working on.&lt;/p&gt;
&lt;p&gt;In psychology, network researchers are often interested in estimating the conditional (in)dependence structure between a set of variables, say PTSD symptoms, using partial correlation networks, or Gaussian graphical models. Typically this involves a data-driven model selection stage where partial correlations are set to zero either through a hypothesis test or some form of regularization, &lt;em&gt;and then&lt;/em&gt; parameters are estimated. As previously mentioned, this can have a serious effect on inference. To show this effect, I will check the coverage rates and sampling distributions for a parameter of interest with and without selecting a model. Namely, I use the sum of partial correlations, or “edges”, for a given variable (i.e., “expected influence”).&lt;/p&gt;
&lt;p&gt;To do so, I used the &lt;code&gt;R&lt;/code&gt; package &lt;a href=&#34;https://github.com/donaldRwilliams/BGGM&#34;&gt;BGGM&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(Williams &amp;amp; Mulder, &lt;a href=&#34;#ref-williamsBayesian2019a&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; to estimate networks with and without a model selection stage. This package estimates networks under a Bayesian framework so credible intervals were computed instead of confidence intervals. Although credible intervals have a different interpretation than confidence intervals,&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; their coverage properties remain the same.&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The simulation steps went as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Randomly sample observations from a multivariate normal distribution&lt;/li&gt;
&lt;li&gt;Estimate the partial correlation network&lt;/li&gt;
&lt;li&gt;If performing model selection, set “insignificant” partial correlations to zero&lt;/li&gt;
&lt;li&gt;Compute expected influence from the posterior distributions of “significant” partial correlations&lt;/li&gt;
&lt;li&gt;Compute a credible interval for the expected influence&lt;/li&gt;
&lt;li&gt;Check whether the interval covered the true value for expected influence&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note, to clearly show the distorted sampling distribution, I kept track of the largest single partial correlation included in the expected influence calculation. The code I used for this simulation can be found at the bottom of this post.&lt;/p&gt;
&lt;div id=&#34;coverage&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coverage&lt;/h2&gt;
&lt;p&gt;After running the simulations, taking the means of &lt;code&gt;covered_condit&lt;/code&gt; and &lt;code&gt;covered_full&lt;/code&gt; yields the proportion of times that the credible interval covered the true parameter value when a model was selected and when it was not.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://josue.rbind.io/post/model-selection-bias_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is clear to see from this plot that when a model is not selected, the coverage rate is just about what we would expect — approximately 90 in every 100 credible intervals contained the true value for expected influence. But, when the model was selected prior to computing this interval, the true value was covered much less frequently. Only about 70 in 100 times!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sampling-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sampling Distribution&lt;/h2&gt;
&lt;p&gt;Recall that model selection also distorts the sampling distribution of parameters. In the simulations, I kept track of the partial correlation between variables 1 and 6. On each simulation trial, I simply took the mean of the posterior distribution for this parameter. While we would expect to see the means normally distributed around the true value (red line), the distribution is truncated when a model is selected.&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; This results in overconfident inferences about the population value. Also notice that parameters estimated after model selection are biased towards large effects. This makes sense as larger effects are more likely to be “significant”.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://josue.rbind.io/post/model-selection-bias_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The simulations and plots are simple, but they convey a powerful idea. Model selection distorts inferential properties such as coverage rates and sampling distributions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;I’d like to emphasize that what I have briefly discussed in this post should not be confused with uncertainty in the model selection process,&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; but rather that drawing &lt;em&gt;valid&lt;/em&gt; post-model-selection inference is (almost) impossible.&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As mentioned above, researchers often estimate parameters after selecting parameters, for example, expected influence. However, as we saw, this invalidates inferential procedures. A simple remedy would be to simply forgo the model selection process and estimate this parameter with the full model. Inference can then be conducted on the parameters of interest. If one &lt;em&gt;must&lt;/em&gt; select a model, a straightforward solution is to perform data-splitting. That is, split your data into two parts, use one part for model selection, and use the other for inference. In fact, according to Cosma Shalizi, “Sample splitting is a simple, radical, almost a-theoretical way to solve the problem of post-selection inference…”.&lt;/p&gt;
&lt;p&gt;Moreover, it may be tempting to think that because Bayesian inference is conditional on the data, issues with post-selection inference does not apply. This is not so (as shown in the plots above). Model selection with Bayesian methods leads to a selective posterior due to a truncated likelihood &lt;span class=&#34;citation&#34;&gt;(Panigrahi et al., &lt;a href=&#34;#ref-panigrahiIntegrative2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;; Panigrahi &amp;amp; Taylor, &lt;a href=&#34;#ref-panigrahiScalable2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;I’d like to conclude with a nice cautionary quote&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We do not discuss statistical inference…(e.g., looking at &lt;em&gt;p&lt;/em&gt;-values associated with each predictor). If you do wish to look at the statistical significance of the predictors, beware that any procedure involving selecting predictors &lt;em&gt;first&lt;/em&gt; will invalidate the assumptions behind the &lt;em&gt;p&lt;/em&gt;-values. The [data-driven] procedures we recommend for selecting predictors are helpful when the model is used for [prediction]; they are not helpful if you wish to study the effect of any predictor &lt;span class=&#34;citation&#34;&gt;(Hyndman &amp;amp; Athanasopoulos, &lt;a href=&#34;#ref-hyndman_2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;, p. 168)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#==========
# Set up 
#==========
set.seed(24)

# number of variables
p &amp;lt;- 10
# number of (partial) cors
len_out &amp;lt;- p * (p - 1) / 2 
# values for cors
sample_space &amp;lt;- seq(0.1, 0.4, by = 0.001)
rhos &amp;lt;- sample(sample_space, size = len_out, replace = TRUE) 

# covariance matrix
S &amp;lt;- matrix(1, p, p)
S[lower.tri(S)] &amp;lt;- rhos
S[upper.tri(S)] &amp;lt;- t(S)[upper.tri(S)]
# partial cor matrix
true_pcors &amp;lt;- -cov2cor(solve(S))
# true expected influence
true_ei &amp;lt;- sum(true_pcors[1, -1])
# [1] 0.7456753

# true partial cor between 1 and 6
true_pcors[1, 6]
# [1] 0.1811494

# sample size for each trial
n &amp;lt;- 100

# number of trials
niter &amp;lt;- 1500

#============================
# Conditional selection
#============================

# containers
covered_condit &amp;lt;- mean_condit_ei &amp;lt;- rep(NA, niter)

for (iter in 1:niter) {
  # sample data and fit GGM
  Y &amp;lt;- MASS::mvrnorm(n, rep(0, p), S)
  fit &amp;lt;- estimate(Y, iter = 1000, progress = FALSE)
  sel &amp;lt;- select(fit)
  
  # if none are selected move on to next iteration
  if (all(sel$adj[1, ] == 0)) {
    next
  }
  
  # select &amp;quot;significant&amp;quot; nodes
  selected_pcors &amp;lt;- array(NA, dim = dim(sel$object$post_samp$pcors))
  selected_pcors[] &amp;lt;- apply(sel$object$post_samp$pcors, 3, function(x) sel$adj * x)
  
  # true value of sum for selected nodes
  true_conditional &amp;lt;- sum(true_pcors[1, ] * sel$adj[1, ])
  
  # compute expected influence for selected nodes
  ei &amp;lt;- colSums(selected_pcors[1,,])
  
  # compute 90% credible interval
  cri &amp;lt;- quantile(ei, probs = c(0.05, 0.95) )
  
  # check if true sum is covered in credible interval
  covered_condit[iter] &amp;lt;- ifelse(cri[1] &amp;lt; true_conditional &amp;amp; 
                                   cri[2] &amp;gt; true_conditional, 1, 0)
  
  # keep track of particular relationship to plot sampling distribution
  if (sel$adj[1, 6] == 1) {
    mean_condit_ei[iter] &amp;lt;- mean(selected_pcors[1, 6, ])
  }
}


#============================
# Unconditional selection
#============================

# containers
covered_full &amp;lt;- mean_full_ei &amp;lt;- rep(NA, niter)

# full model, no selection
for (iter in 1:niter){
  Y &amp;lt;- MASS::mvrnorm(n, rep(0, p), S)
  fit &amp;lt;- estimate(Y, iter = 1000, progress = FALSE)
  
  ei &amp;lt;- colSums(fit$post_samp$pcors[1,,])
  
  cri &amp;lt;- quantile(ei, probs = c(0.05, 0.95) )

  mean_full_ei[iter] &amp;lt;- mean(fit$post_samp$pcors[1, 6, ])
  
  covered_full[iter] &amp;lt;- ifelse(cri[1] &amp;lt; true_ei &amp;amp; 
                                 true_ei &amp;lt; cri[2], 1, 0)
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;acknowledgments&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Acknowledgments&lt;/h4&gt;
&lt;p&gt;I would like to thank &lt;a href=&#34;https://twitter.com/wdonald_1985&#34;&gt;Donny Williams&lt;/a&gt; for motivating me to write this blog post and providing tons of helpful feedback.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-berkValid2013&#34;&gt;
&lt;p&gt;Berk, R., Brown, L., Buja, A., Zhang, K., &amp;amp; Zhao, L. (2013). Valid post-selection inference. &lt;em&gt;Annals of Statistics&lt;/em&gt;, &lt;em&gt;41&lt;/em&gt;(2), 802–837. &lt;a href=&#34;https://doi.org/10.1214/12-AOS1077&#34;&gt;https://doi.org/10.1214/12-AOS1077&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hyndman_2018&#34;&gt;
&lt;p&gt;Hyndman, R. J., &amp;amp; Athanasopoulos, G. (2018). &lt;em&gt;Forecasting: Principles and practice&lt;/em&gt; (Second, p. 168). OTexts.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-panigrahiScalable2017&#34;&gt;
&lt;p&gt;Panigrahi, S., &amp;amp; Taylor, J. (2017). Scalable methods for Bayesian selective inference. &lt;em&gt;arXiv:1703.06176 [Stat]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/1703.06176&#34;&gt;http://arxiv.org/abs/1703.06176&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-panigrahiIntegrative2020&#34;&gt;
&lt;p&gt;Panigrahi, S., Taylor, J., &amp;amp; Weinstein, A. (2020). Integrative Methods for Post-Selection Inference Under Convex Constraints. &lt;em&gt;arXiv:1605.08824 [Stat]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/1605.08824&#34;&gt;http://arxiv.org/abs/1605.08824&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-williamsBayesian2019a&#34;&gt;
&lt;p&gt;Williams, D. R., &amp;amp; Mulder, J. (2019). &lt;em&gt;Bayesian Hypothesis Testing for Gaussian Graphical Models: Conditional Independence and Order Constraints&lt;/em&gt; [Preprint]. PsyArXiv. &lt;a href=&#34;https://doi.org/10.31234/osf.io/ypxd8&#34;&gt;https://doi.org/10.31234/osf.io/ypxd8&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Other fields as well but I’m in psychology so that is my focus&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;One could argue that this is not an issue with a consistent model selector or large enough sample sizes, but see &lt;span class=&#34;citation&#34;&gt;(&lt;span class=&#34;citeproc-not-found&#34; data-reference-id=&#34;leebMODEL2005&#34;&gt;&lt;strong&gt;???&lt;/strong&gt;&lt;/span&gt;)&lt;/span&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;If you have any interest in network models at all I highly suggest checking out the BGGM package! It offers a variety of flexible methods for both exploratory and confirmatory analyses plus it now handles ordinal data and VAR models&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;A 90% credible interval can literally be interpreted as a 90% probability that the true parameter value is covered, given the data&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;“frequentist properties of Bayesian methods” is a good google search if you’re bored&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;There are methods to correct this truncation, but this is still an active area of research&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;To account for uncertainty in model selection some propose model averaging&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;There is a growing body of literature on “undoing” model selection bias, but it is not yet a mature body of literature&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tutorial: Bayesian Testing of Central Structures in Psychological Networks</title>
      <link>https://josue.rbind.io/post/tutorial-bayesian-testing/</link>
      <pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate>
      <guid>https://josue.rbind.io/post/tutorial-bayesian-testing/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This tutorial demonstrates how to use networks (specifically Gaussian graphical models or GGMs) to&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Generate hypotheses&lt;/li&gt;
&lt;li&gt;Perform confirmatory tests on the generated hypotheses&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- In psychology, network models are almost never used to generate hypotheses. This is puzzling because one of the original reasons researchers began using them was exactly this. Networks are also thought of as highly exploratory tools, despite their potential for confirmatory tests.  --&gt;
&lt;p&gt;Network theory has emerged as a popular framework for conceptualizing psychological constructs and mental disorders. Initially, network analysis was motivated in part by the thought that it can be used for hypothesis generation. Although the customary approach for network modeling is inherently exploratory, we argue that there is untapped potential for confirmatory hypothesis testing. These ideas are expanded upon in our recent paper, “On Formalizing Theoretical Expectations: Bayesian Testing of Central Structures in Pyschological Networks”, where we merge exploratory and confirmatory hypotheses into a cohesive framework based on Bayesian hypothesis testing. You can find the preprint &lt;a href=&#34;https://psyarxiv.com/zw7pf/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In what follows, I will describe how you can use &lt;code&gt;R&lt;/code&gt; to perform confirmatory hypothesis tests based on initial, exploratory hypotheses with GGMs. For clarity, some code chunks have been omitted, but the full code to reproduce this document can be obtained on the &lt;a href=&#34;https://osf.io/bh783&#34;&gt;Open Science Framework&lt;/a&gt; or &lt;a href=&#34;https://github.com/josue-rodriguez/formalizing-expectations&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Examples&lt;/h1&gt;
&lt;div id=&#34;ptsd-network&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PTSD Network&lt;/h2&gt;
&lt;p&gt;To begin we need several packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;BGGM&lt;/code&gt;: to conduct exploratory and confirmatory analyses with GGMs&lt;/li&gt;
&lt;li&gt;&lt;code&gt;MASS&lt;/code&gt;: to generate data from covariance matrices&lt;/li&gt;
&lt;li&gt;&lt;code&gt;networktools&lt;/code&gt;: to calculate bridge centrality statistics&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Uncomment and run  if missing packages

# for latest version of BGGM
# remotes::install_github(&amp;#39;donaldrwilliams/BGGM&amp;#39;) 

# packages &amp;lt;- c(&amp;quot;MASS&amp;quot;, &amp;quot;networktools&amp;quot;)
# if (!packages %in% installed.packages()) install.packages(packages)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MASS)
library(networktools)
library(BGGM)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first dataset contains measurements on 16 PTSD symptoms in 3 communities, “Re-experiencing”, “Avoidance”, and “Arousal” &lt;span class=&#34;citation&#34;&gt;(Sample 4 in Fried et al., &lt;a href=&#34;#ref-friedReplicability2018a&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;. Only the correlations matrices are available so we have to generate the data using &lt;code&gt;MASS::mvrnorm&lt;/code&gt; with &lt;code&gt;empirical = TRUE&lt;/code&gt;. This ensures that the generated data have the exact covariance structure we give it. The correlation matrix, &lt;code&gt;ptsd_cor4&lt;/code&gt;, is loaded with &lt;code&gt;BGGM&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1812)

# data for exploratory analyses
explore_ptsd &amp;lt;- mvrnorm(n = 965, 
                        mu = rep(0, 16), 
                        Sigma = ptsd_cor4, 
                        empirical = TRUE)
colnames(explore_ptsd) &amp;lt;- node_names&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;exploratory-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploratory analysis&lt;/h3&gt;
&lt;div id=&#34;estimate-graph&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Estimate graph&lt;/h4&gt;
&lt;p&gt;The approach begins by estimating an exploratory network. With &lt;code&gt;BGGM&lt;/code&gt;, this requires calling the &lt;code&gt;explore&lt;/code&gt; function to obtain and sample the posterior distribution. These results are saved in &lt;code&gt;explore_network&lt;/code&gt;. The &lt;code&gt;select&lt;/code&gt; function takes the results from an &lt;code&gt;explore&lt;/code&gt; call, and is used to determine the edge set for &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{A}^{CD}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{A}^{CI}\)&lt;/span&gt; — the conditional (in)dependence structures. We used &lt;code&gt;type = &#34;continuous&#34;&lt;/code&gt; as the data we generated are multivariate normal, but the latest version &lt;code&gt;BGGM&lt;/code&gt; can also handle ordinary and binary data.&lt;/p&gt;
&lt;p&gt;Note that &lt;code&gt;alternative = &#34;exhaustive&#34;&lt;/code&gt; is used for determining the edge set. This returns three adjacency matrices. One for positive, negative, and null edges. In the paper, we focus on positive and null relations. The former is due to the expectation that edges in psychological networks are expected to be positive (i.e., a positive manifold).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sample posterior distribution
explore_network &amp;lt;- explore(explore_ptsd, 
                           type = &amp;quot;continuous&amp;quot;,
                           iter = 5000)

# determine edge set
selected_network &amp;lt;- select(explore_network, 
                           alternative = &amp;quot;exhaustive&amp;quot;, 
                           bf_cut = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bridge-centrality&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Bridge Centrality&lt;/h4&gt;
&lt;p&gt;The next step is to calculate bridge strength using the &lt;code&gt;networktools&lt;/code&gt; package. This is similar to node strength, in that, for a given node, it is the sum of the absolute values of its edges. However, bridge strength only takes into account edges that connect to different communities, or clusters. Thus, it is a measure of inter-community connectivity, and can be used to identify central structures in a network.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# retain positive partial correlations from selected network
partial_cors &amp;lt;- selected_network$pcor_mat * selected_network$pos_mat

# rename columns with node names
dimnames(partial_cors) &amp;lt;- list(node_names, node_names)

# calculate bridge strength. comms is a vector specifying 
# the community for each node
bridge_strengths &amp;lt;- bridge(partial_cors, communities = comms)$`Bridge Strength`

# we use the top 10% in bridge strength as bridge nodes
bridge_strength_cutoff &amp;lt;- quantile(bridge_strengths, 0.9)
bridge_strengths[bridge_strengths &amp;gt; bridge_strength_cutoff]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    B4    D1 
## 0.532 0.657&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that the partial correlation matrix was multiplied by adjacency matrix for partial correlations (&lt;code&gt;selected_network$pos_mat&lt;/code&gt;). This ensures we are only focusing on positive associations.&lt;/p&gt;
&lt;p&gt;Calculating bridge strength indicates that nodes B4 and D1 are the top bridge nodes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-bridges&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Plot bridges&lt;/h4&gt;
&lt;p&gt;A key idea in our paper was that highlighting and “zooming” in on central structures allows researchers to easily formulate hypotheses. For example, we plotted the neighborhood of bridge relations for nodes D1 and B4.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://josue.rbind.io/post/tutorial-bayesian-testing_files/figure-html/plot-ptsd-bridges-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://josue.rbind.io/post/tutorial-bayesian-testing_files/figure-html/plot-ptsd-bridges-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;confirmatory-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Confirmatory analysis&lt;/h3&gt;
&lt;p&gt;With the central structures identified and plotted, we can then move on to formulating and testing hypotheses.&lt;/p&gt;
&lt;p&gt;For this confirmatory analysis, data were generated from another correlation matrix &lt;span class=&#34;citation&#34;&gt;(Sample 3 in Fried et al., &lt;a href=&#34;#ref-friedReplicability2018a&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;. This was done to test our hypotheses on a different dataset than the one used for exploratory analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

confirm_ptsd &amp;lt;- mvrnorm(n = 926, 
                        mu = rep(0, 16), 
                        Sigma = ptsd_cor3, 
                        empirical = TRUE)

colnames(confirm_ptsd) &amp;lt;- node_names&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;varying-degrees-of-replication&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Varying degrees of replication&lt;/h4&gt;
&lt;p&gt;We first focus on node B4 and test the following hypotheses
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
    \mathcal{H}_1&amp;amp;: (\rho_{B4-C1}, \rho_{B4-C7}, \rho_{B4-D3},  \rho_{B4-D4}) &amp;gt; 0 \\ \nonumber
    \mathcal{H}_2&amp;amp;: \rho_{B4-C1} &amp;gt; (\rho_{B4-C7}, \rho_{B4-D3}, \rho_{B4-D4}) &amp;gt; 0 \\ \nonumber
    \mathcal{H}_3 &amp;amp;: ``\text{not}\; \mathcal{H}_1 \; \text{or}\; \mathcal{H}_2  \text{.&amp;#39;&amp;#39;}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Above, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt; is testing for replication of all edges but is otherwise agnostic towards the interplay among bridge relations. &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_2\)&lt;/span&gt; then provides a refined view into the bridge neighborhood by testing an additional constraint that the strongest edge replicated. That is, all of the bridge relations &lt;em&gt;and&lt;/em&gt; the strongest edge re-emerged in an independent dataset. Furthermore, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_2\)&lt;/span&gt; both reflect a positive manifold. We also included &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_3\)&lt;/span&gt; which accounts for structures that are not &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To test these hypotheses, we can write them out in a single string and use the &lt;code&gt;confirm&lt;/code&gt; function. Note that hypotheses are separated by a semicolon, and that partial correlations are denoted as &lt;code&gt;node1 -- node2&lt;/code&gt;. The output is obtained by simply printing out the results of &lt;code&gt;confirm&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hyp_var_rep &amp;lt;- c(&amp;quot;(B4--C1, B4--C7, B4--D3, B4--D4) &amp;gt; 0;
                   B4--C1 &amp;gt; (B4--C7, B4--D3, B4--D4) &amp;gt; 0&amp;quot;)

confirm_var_rep &amp;lt;- confirm(confirm_ptsd,
                           hyp_var_rep,
                           iter = 50000)
confirm_var_rep&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## BGGM: Bayesian Gaussian Graphical Models 
## Type: continuous 
## --- 
## Posterior Samples: 50000 
## Observations (n): 926 
## Variables (p): 16 
## Delta: 15 
## --- 
## Call:
## confirm(Y = confirm_ptsd, hypothesis = hyp_var_rep, iter = 50000)
## --- 
## Hypotheses: 
## 
## H1: (B4--C1,B4--C7,B4--D3,B4--D4)&amp;gt;0
## H2: 
## B4--C1&amp;gt;(B4--C7,B4--D3,B4--D4)&amp;gt;0
## H3: complement
## --- 
## Posterior prob: 
## 
## p(H1|data) = 0.197
## p(H2|data) = 0.797
## p(H3|data) = 0.006
## --- 
## Bayes factor matrix: 
##       H1    H2      H3
## H1 1.000 0.247  33.401
## H2 4.054 1.000 135.404
## H3 0.030 0.007   1.000
## --- 
## note: equal hypothesis prior probabilities&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output includes both the posterior probabilities and all of the Bayes factors. The Bayes factors are in reference to the rows relative to the columns. For example the element in the 2nd row and 1st column would be interpreted as &lt;span class=&#34;math inline&#34;&gt;\(\text{BF}_{21} = 4.05\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this case, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_2\)&lt;/span&gt; is the preferred hypothesis, that is, all of the bridge edges and the strongest edge replicated. This gets at an important notion. It is possible to test &lt;em&gt;varying degrees of replication&lt;/em&gt;.&lt;/p&gt;
&lt;!-- This analysis also indicates that (1) the bridge relations replicated in an independent dataset; and (2) the relation between ``sleep problems&#39;&#39; (node B4) and ``avoidance of thoughts&#39;&#39; (node C1) \emph{could} be the strongest bridge between the Re-experiencing and Avoidance communities. --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;comorbidity-network&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comorbidity Network&lt;/h2&gt;
&lt;p&gt;We also examined a comorbity network containing 16 symptoms of anxiety and depression &lt;span class=&#34;citation&#34;&gt;(Beard et al., &lt;a href=&#34;#ref-beardNetwork2016&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;, and followed the same steps as above. This dataset is available on the &lt;a href=&#34;https://osf.io/wemcg/&#34;&gt;OSF&lt;/a&gt;. Here, however, we split the data into two because we did not have independent datasets. We formulated hypotheses on one half and tested them on the remaining half.&lt;/p&gt;
&lt;div id=&#34;exploratory-analysis-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploratory analysis&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(27)

cov_anxdep &amp;lt;- read.csv(&amp;quot;../05-data/00-cov-anxdep.csv&amp;quot;)[, -1]
sim_anxdep &amp;lt;- MASS::mvrnorm(n = 1029,
                            mu = rep(0, 16), 
                            Sigma = cov_anxdep,
                            empirical = TRUE)

split &amp;lt;- sample(1:1029, size = floor(1029 * .5))
explore_anxdep &amp;lt;- sim_anxdep[split, ]
confirm_anxdep &amp;lt;- sim_anxdep[-split, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;explore_network &amp;lt;- explore(explore_anxdep, 
                           type = &amp;quot;continuous&amp;quot;,
                           iter = 5000)

selected_network &amp;lt;- select(explore_network, 
                           alternative = &amp;quot;exhaustive&amp;quot;, 
                           BF_cut = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;bridge-strength&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Bridge Strength&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;partial_cors &amp;lt;- selected_network$pcor_mat * selected_network$pos_mat

partial_cors &amp;lt;- round(partial_cors, 4)
dimnames(partial_cors) &amp;lt;- list(node_names, node_names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bridge_strengths &amp;lt;- bridge(partial_cors, communities = comms)$`Bridge Strength`
bridge_strength_cutoff &amp;lt;- quantile(bridge_strengths, 0.9)
bridge_strengths[bridge_strengths &amp;gt; bridge_strength_cutoff]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    D6    D8 
## 0.270 0.404&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-bridges-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Plot bridges&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://josue.rbind.io/post/tutorial-bayesian-testing_files/figure-html/plot-anxdep-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;confirmatory-analysis-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Confirmatory analysis&lt;/h3&gt;
&lt;div id=&#34;intra--and-inter-bridge-sets&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Intra- and Inter-Bridge Sets&lt;/h4&gt;
&lt;p&gt;The following hypotheses focus on characterizing bridge sets, or the set of bridge edges belonging to a given symptom. For example, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt; posits that the bridge set for node D8 is collectively greater than the set for node D6, with the constraint that the edges within the bridge set for D8 are equal to each other. This effectively corresponds to testing whether node D8 has greater bridge strength than node D6. &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_2\)&lt;/span&gt; then refines &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt; by testing an exact order both between and within bridge sets.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\label{eq:intra-inter}
    \mathcal{H}_1 &amp;amp;: \rho_{D8-A5} = \rho_{D8-A7} &amp;gt; (\rho_{D6-A3}, \rho_{D6-A6}) &amp;gt; 0 \\ \nonumber
    \mathcal{H}_2 &amp;amp;: \rho_{D8-A5} &amp;gt; \rho_{D8-A7} &amp;gt; \rho_{D6-A3} &amp;gt; \rho_{D6-A6} &amp;gt; 0 \\ \nonumber
    \mathcal{H}_3 &amp;amp;: ``\text{not}\; \mathcal{H}_1 \; \text{or}\; \mathcal{H}_2 \text{.&amp;#39;&amp;#39;}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that the inclusion of an inequality and equality constraint in a single hypothesis is currently only available on the GitHub version for &lt;code&gt;BGGM&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;intra_inter_hyp &amp;lt;- c(&amp;quot;D8--A5 = D8--A7 &amp;gt; (D6--A3, D6--A6) &amp;gt; 0;
                      D8--A5 &amp;gt; D8--A7 &amp;gt; D6--A3 &amp;gt; D6--A6 &amp;gt; 0&amp;quot;)

confirm_intra_inter &amp;lt;- confirm(Y = confirm_anxdep,
                               hypothesis = intra_inter_hyp,
                               iter = 50000)
confirm_intra_inter &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## BGGM: Bayesian Gaussian Graphical Models 
## Type: continuous 
## --- 
## Posterior Samples: 50000 
## Observations (n): 515 
## Variables (p): 16 
## Delta: 15 
## --- 
## Call:
## confirm(Y = confirm_anxdep, hypothesis = intra_inter_hyp, iter = 50000)
## --- 
## Hypotheses: 
## 
## H1: D8--A5=D8--A7&amp;gt;(D6--A3,D6--A6)&amp;gt;0
## H2: 
## D8--A5&amp;gt;D8--A7&amp;gt;D6--A3&amp;gt;D6--A6&amp;gt;0
## H3: complement
## --- 
## Posterior prob: 
## 
## p(H1|data) = 0.037
## p(H2|data) = 0.954
## p(H3|data) = 0.009
## --- 
## Bayes factor matrix: 
##        H1    H2      H3
## H1  1.000 0.039   4.251
## H2 25.905 1.000 110.115
## H3  0.235 0.009   1.000
## --- 
## note: equal hypothesis prior probabilities&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data were more likely under both &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(\text{BF}_{13} = 4.3\)&lt;/span&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_2\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(\text{BF}_{23} = 110.1\)&lt;/span&gt;) than &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_3\)&lt;/span&gt;. Furthermore, there was more evidence supporting the hypothesis testing solely inequality constraints, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_2\)&lt;/span&gt;, than the one including an equality constraint (&lt;span class=&#34;math inline&#34;&gt;\(\text{BF}_{21} = 25.9\)&lt;/span&gt;). This provides a clear characterization of the the bridge relations at hand — not only did the order of bridge strength replicate, but so did the order of the edges within each bridge set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bridge-set-separation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Bridge Set Separation&lt;/h4&gt;
&lt;p&gt;We also thought it would be interesting to test whether bridge sets include common elements. That is, whether bridge symptoms connect to the same or different nodes. As can be seen above, the bridge sets for nodes D8 and D6 are mutually exclusive. We can then test, say, whether D8 is conditionally independent from the bridge set for D6 (nodes A3 and A6)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
    \mathcal{H}_1 &amp;amp;: (\rho_{D8-A3}, \rho_{D8-A6}) = 0 \\ \nonumber
    \mathcal{H}_2 &amp;amp;: (\rho_{D8-A3}, \rho_{D8-A6})  &amp;gt; 0 \\ \nonumber
    \mathcal{H}_3 &amp;amp;: ``\text{not}\; \mathcal{H}_1 \; \text{or} \; \mathcal{H}_2 \text{.&amp;#39;&amp;#39;}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hyp_bridge_sep &amp;lt;- (&amp;quot;(D8--A3, D8--A6) = 0;
                    (D8--A3, D8--A6) &amp;gt; 0&amp;quot;)

confirm_bridge_sep &amp;lt;- confirm(Y = confirm_anxdep,
                              hypothesis = hyp_bridge_sep,
                              iter = 50000)
confirm_bridge_sep&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## BGGM: Bayesian Gaussian Graphical Models 
## Type: continuous 
## --- 
## Posterior Samples: 50000 
## Observations (n): 515 
## Variables (p): 16 
## Delta: 15 
## --- 
## Call:
## confirm(Y = confirm_anxdep, hypothesis = hyp_bridge_sep, iter = 50000)
## --- 
## Hypotheses: 
## 
## H1: (D8--A3,D8--A6)=0
## H2: 
## (D8--A3,D8--A6)&amp;gt;0
## H3: complement
## --- 
## Posterior prob: 
## 
## p(H1|data) = 0.307
## p(H2|data) = 0.61
## p(H3|data) = 0.084
## --- 
## Bayes factor matrix: 
##       H1    H2    H3
## H1 1.000 0.503 3.665
## H2 1.989 1.000 7.288
## H3 0.273 0.137 1.000
## --- 
## note: equal hypothesis prior probabilities&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although the data were more likely under &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt; than &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_3\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(\text{BF}_{13} = 3.7\)&lt;/span&gt;), there was support in favor of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_2\)&lt;/span&gt; compared to &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(\text{BF}_{21} = 2\)&lt;/span&gt;). This analysis suggests it is unlikely that D8 is actually conditionally independent from the bridge set for D6.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this tutorial, I briefly described a framework to formulate and test hypotheses on psychological networks using &lt;code&gt;BGGM&lt;/code&gt;. The idea is to (1) identify central structures on which you can formulate hypotheses in an exploratory analysis and (2) test those hypotheses on independent data. This brings networks to fruition as tools to generate hypotheses and overcomes the idea that they are solely for exploratory purposes.&lt;/p&gt;
&lt;p&gt;In writing the above paper, our hope is that researchers can integrate these methods into their work. We believe that conducting confirmatory tests is an important step forward in psychological networks.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-beardNetwork2016&#34;&gt;
&lt;p&gt;Beard, C., Millner, A. J., Forgeard, M. J. C., Fried, E. I., Hsu, K. J., Treadway, M. T., Leonard, C. V., Kertz, S. J., &amp;amp; Björgvinsson, T. (2016). Network analysis of depression and anxiety symptom relationships in a psychiatric sample. &lt;em&gt;Psychological Medicine&lt;/em&gt;, &lt;em&gt;46&lt;/em&gt;(16), 3359–3369. &lt;a href=&#34;https://doi.org/10.1017/S0033291716002300&#34;&gt;https://doi.org/10.1017/S0033291716002300&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-friedReplicability2018a&#34;&gt;
&lt;p&gt;Fried, E. I., Eidhof, M. B., Palic, S., Costantini, G., Huisman-van Dijk, H. M., Bockting, C. L. H., Engelhard, I., Armour, C., Nielsen, A. B. S., &amp;amp; Karstoft, K.-I. (2018). Replicability and Generalizability of Posttraumatic Stress Disorder (PTSD) Networks: A Cross-Cultural Multisite Study of PTSD Symptoms in Four Trauma Patient Samples. &lt;em&gt;Clinical Psychological Science&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;(3), 335–351. &lt;a href=&#34;https://doi.org/10.1177/2167702617745092&#34;&gt;https://doi.org/10.1177/2167702617745092&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
