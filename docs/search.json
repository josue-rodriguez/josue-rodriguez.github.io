[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "posts",
    "section": "",
    "text": "(simple) Linear regression\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2024\n\n\nJosue Rodriguez\n\n\n\n\n\n\n\n\n\n\n\n\nModel Selection Bias\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2020\n\n\nJosue Rodriguez\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "josue rodriguez",
    "section": "",
    "text": "Welcome! My name is Josue (pronounced ho-sway).\nI’m a data scientist at McGraw Hill where I’m helping build personalized learning experiences at scale. Previously, I completed my PhD in Quantitative Psychology at UC Davis where I focused on developing Bayesian statistical models for social-behavior science."
  },
  {
    "objectID": "posts/2020-6-18-model-selection-bias/model-selection-bias.html",
    "href": "posts/2020-6-18-model-selection-bias/model-selection-bias.html",
    "title": "Model Selection Bias",
    "section": "",
    "text": "Over the last few months, a frequent topic of conversation with my lab mate Donny has been the issue of valid inference following model selection, or model selection bias. This problem has been recognized since at least 1963 and has been written about extensively since then. Some resources I have found both helpful and accessible in understanding model selection bias can be found here, here, and here. However, this issue is still pervasive among social and behavioral scientists 1, so I am writing a short post here in hopes of clarifying the ramifications of drawing inference after selecting a model."
  },
  {
    "objectID": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#why-does-this-occur",
    "href": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#why-does-this-occur",
    "title": "Model Selection Bias",
    "section": "Why does this occur?",
    "text": "Why does this occur?\nAn assumption behind the majority of inferential procedures is that the model is fixed, but model selection makes the model itself random. Inferential procedures typically do not account for this stochastic aspect. Using confidence intervals, let’s take a second to think about why model selection introduces randomness.\nWikipedia defines a 90% confidence interval in terms of sampling by stating\n\nWere this procedure to be repeated on numerous samples, the fraction of calculated confidence intervals (which would differ for each sample) that encompass the true population parameter would tend toward 90%.”\n\nWhat I would like to emphasize here is that that valid confidence intervals have a long run guarantee of covering the true population parameter (approximately) 90 out of 100 times, and this guarantee is based on carrying out the same procedure repeatedly. In this context, procedure refers to an estimation procedure and implies that the statistical model used for inference does not change from sample to sample.\nNow, suppose we have collected some data and we have some candidate set of predictors. We then pick a subset of these predictors according to some model selection procedure and estimate a model with this subset. The end result is an estimation procedure that is conditional on the selected variables.\nIf we do this repeatedly — collect some new data, select a model, and fit it with the selected predictors — there is no guarantee that the estimation procedure will have the same predictors each time 2. It is this randomness introduced by model selection that invalidates the properties of classical inference."
  },
  {
    "objectID": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#coverage",
    "href": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#coverage",
    "title": "Model Selection Bias",
    "section": "Coverage",
    "text": "Coverage\nAfter running the simulations, taking the means of covered_condit and covered_full yields the proportion of times that the credible interval covered the true parameter value when a model was selected and when it was not.\n\n\n\n\n\n\n\n\n\nIt is clear to see from this plot that when a model is not selected, the coverage rate is just about what we would expect — approximately 90 in every 100 credible intervals contained the true value for expected influence. But, when the model was selected prior to computing this interval, the true value was covered much less frequently. Only about 70 in 100 times!"
  },
  {
    "objectID": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#sampling-distribution",
    "href": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#sampling-distribution",
    "title": "Model Selection Bias",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\nRecall that model selection also distorts the sampling distribution of parameters. In the simulations, I kept track of the partial correlation between variables 1 and 6. On each simulation trial, I simply took the mean of the posterior distribution for this parameter. While we would expect to see the means normally distributed around the true value (red line), the distribution is truncated when a model is selected 6. This results in overconfident inferences about the population value. Also notice that parameters estimated after model selection are biased towards large effects. This makes sense as larger effects are more likely to be “significant”.\n\n\n\n\n\n\n\n\n\nThe simulations and plots are simple, but they convey a powerful idea. Model selection distorts inferential properties such as coverage rates and sampling distributions."
  },
  {
    "objectID": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#footnotes",
    "href": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#footnotes",
    "title": "Model Selection Bias",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOther fields as well but I’m in psychology so that is my focus↩︎\nOne could argue that this is not an issue with a consistent model selector or large enough sample sizes, but see Leeb & Pötscher (2005)↩︎\nIf you have any interest in network models at all I highly suggest checking out the BGGM package! It offers a variety of flexible methods for both exploratory and confirmatory analyses plus it now handles ordinal data and VAR models↩︎\nA 90% credible interval can literally be interpreted as a 90% probability that the true parameter value is covered, given the data↩︎\n“frequentist properties of Bayesian methods” is a good google search if you’re bored↩︎\nThere are methods to correct this truncation, but this is still an active area of research↩︎\nTo account for uncertainty in model selection some propose model averaging↩︎\nThere is a growing body of literature on “undoing” model selection bias, but it is not yet a mature body of literature↩︎"
  },
  {
    "objectID": "posts/2024-09-15-simple-linear-regression/index.html",
    "href": "posts/2024-09-15-simple-linear-regression/index.html",
    "title": "(simple) Linear regression",
    "section": "",
    "text": "It’s been a couple years since I’ve taught intro to stats to the incoming grad students in the Psychology department at UC Davis, and it’s probably the only thing I really find myself missing about graduate school. One of my favorite parts of teaching was the fact that I got to revisit the basics of statistics every year. It was a chance to look at the foundations through the lens of whatever I had learned in the past year. I don’t miss writing as much, but I suppose that I miss that too.\nWith that in my mind, I want to start writing again, and I’ll start by writing series of short blog posts covering the basics of statistics with the intentions of brushing up on the basics, and indulging my writing whims. Eventually I hope to get to writing posts as a form of learning and retaining new topics. Until then though…I will consider these “warm ups” – they won’t necessarily be thorough, complete, or accurate.1"
  },
  {
    "objectID": "posts/2024-09-15-simple-linear-regression/index.html#r2",
    "href": "posts/2024-09-15-simple-linear-regression/index.html#r2",
    "title": "(simple) Linear regression",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nPerhaps the most commonly computed measure of model fit for regression is called \\(R^2\\), and it’s used to obtain a sense of how well the variable \\(x\\) “explains” the variation in \\(y\\). Having values in \\([0,1]\\), an \\(R^2 = 1\\) indicates that \\(x\\) completely explains all the variation in \\(y\\) whereas \\(R^2=0\\) indidcates \\(x\\) does not explain any variation in \\(y\\).\nDespite it’s popularity, \\(R^2\\) is often a poor measure of model fit. A couple reasons being that it increases with the number of predictor variables, regardless of whether those variables have any association to \\(y\\), and it can be arbitrarily low in a situation where we have fit the true data-generating model. For more details (and a chuckle), check out section 3.2 of the regression notes here.\n\\[\n\\begin{align}\nR^2 &= 1 - \\frac{\\text{SSR}}{\\text{SST}} \\\\\n\\text{SST} &= \\sum_{i=1}^n \\left(y_i - \\bar y \\right)^2\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/2024-09-15-simple-linear-regression/index.html#mean-squared-error",
    "href": "posts/2024-09-15-simple-linear-regression/index.html#mean-squared-error",
    "title": "(simple) Linear regression",
    "section": "Mean squared error",
    "text": "Mean squared error\nInstead of settling for \\(R^2\\) or some variant of it, we can inspect the mean-squared error (MSE). As the name implies, the MSE tells how large our (squared) residuals are on average. The MSE takes a value of 0 when \\(x\\) perfectly predicts \\(y\\), and it can be arbitrarily large otherwise. It provides a direct measure of how well \\(x\\) predicts \\(y\\).\n\\[\n\\begin{align}\n\\text{MSE} &= \\frac{\\sum_{i=1}^n \\left(\\hat y_i - y_i\\right)^2}{n} \\\\\n\\text{MSE} &= \\frac1n \\sum_{i=1}^n \\hat \\varepsilon_i\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/2024-09-15-simple-linear-regression/index.html#residual-variance",
    "href": "posts/2024-09-15-simple-linear-regression/index.html#residual-variance",
    "title": "(simple) Linear regression",
    "section": "Residual variance",
    "text": "Residual variance\nAnother quantity of interest is the estimated residual variance \\(\\hat \\sigma^2_{\\varepsilon}\\). This also tells us how much we expect a given \\(y_i\\) to deviate from our regression line. It’s almost computed almost exactly as the MSE, but it differs in its denominator. This difference makes it an unbiased estimator of the true residual variance, \\(\\sigma^2\\)\n\\[\n\\begin{align}\n\\hat \\sigma^2_{\\varepsilon} &= \\frac{\\sum_{i=1}^n \\left(\\hat y_i - y_i\\right)^2}{n - p - 1} \\\\\n&=  \\frac{1}{n-2}\\sum_{i=1}^n \\hat \\varepsilon_i .\n\\end{align}\n\\]\nNote that the degrees of freedom account for the number of parameters we estimated. So the degrees of freedom are given by \\(n-p-1\\), where \\(p\\) is the number of predictor variables and the one comes from accounting for the fact that we estimated an intercept.\nThe residual variance is assumed to be constant across all values of \\(x\\). That is \\(E[\\sigma^2_{\\varepsilon} | x] = \\sigma^2_{\\varepsilon}\\). At least for the data that arises from social-behavioral studies, this is often an unrealistic assumption. Two ways we can avoid this assumption by taking a weighted least-squares approach or by directly modeling the variance."
  },
  {
    "objectID": "posts/2024-09-15-simple-linear-regression/index.html#putting-our-coefficients-to-the-test",
    "href": "posts/2024-09-15-simple-linear-regression/index.html#putting-our-coefficients-to-the-test",
    "title": "(simple) Linear regression",
    "section": "Putting our coefficients to the test",
    "text": "Putting our coefficients to the test\nRight right, so we can assess whether our model is doing it’s job (“explaining” the current variation in \\(y\\) or predicting new values in \\(y\\)). BUT, what if we wanted to say something about whether \\(\\beta_1\\) was statistically distinguishable from \\(0\\)? That is, what if we wanted to say that the “effect” of \\(x\\) is non-zero?\nThe coefficient \\(\\beta_1\\) is a random variable, and so we can calculate it’s standard error, and consequently, we can perform a hypothesis test for the following hypotheses\n\\[\n\\begin{aligned}\nH_0: \\beta_1 &= \\beta \\\\\n& \\text{vs.} \\\\\nH_1: \\beta_1 & \\neq \\beta\n\\end{aligned}\n\\]\nUsually, \\(\\beta = 0\\), so we test that the regression coefficient for \\(x\\) is equal to 0, but we could pick any ol’ number.\nThe standard error of \\(\\hat \\beta_1\\) is given by\n\\[\n\\begin{aligned}\n\\text{Var}(\\hat \\beta_1) &= \\frac{\\hat \\sigma^2_{\\varepsilon}}{\\sum_{i=1}^n (x_i - \\bar x)^2} \\\\\n\\text{SE}(\\hat \\beta_1) &= \\sqrt{ \\text{Var}(\\hat \\beta_1) }.\n\\end{aligned}\n\\]\nWith the standard error in hand, we can calculate a \\(t\\)-statistic as follows\n\\[\nt = \\frac{\\hat \\beta - \\beta}{\\text{SE}(\\hat \\beta)} %\\sim \\text{Student-}t\\left(\\nu = n-2\\right)\n\\]\nand compute a \\(p\\)-value by referencing it against a \\(t\\)-distribution with degrees of freedom \\(\\nu = n-2\\)."
  },
  {
    "objectID": "posts/2024-09-15-simple-linear-regression/index.html#footnotes",
    "href": "posts/2024-09-15-simple-linear-regression/index.html#footnotes",
    "title": "(simple) Linear regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’d suggest reading one of Gelman’s books or Cosma Shalizi’s book if that’s what you’re looking for↩︎\nHow many processes can you think of that can truly be described this way? 🤔↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "Hi! My name is Josue (pronounced ho-sway).\nI’m a data scientist at McGraw Hill where I’m helping build personalized learning experiences at scale. I previously earned a PhD in Quantitative Psychology at UC Davis, where I focused on developing Bayesian statistical models for social-behavior science."
  },
  {
    "objectID": "about.html#work",
    "href": "about.html#work",
    "title": "about",
    "section": "",
    "text": "Hi! My name is Josue (pronounced ho-sway).\nI’m a data scientist at McGraw Hill where I’m helping build personalized learning experiences at scale. I previously earned a PhD in Quantitative Psychology at UC Davis, where I focused on developing Bayesian statistical models for social-behavior science."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "about",
    "section": " education",
    "text": "education\n\n\n\n  \n    \n      PhD in Quantitative Psychology —\n      2023\n    \n    University of California, Davis\n  \n  \n  \n    \n      MA in Cognitive Psychology —\n      2019\n    \n    Minor in Statistics\n    California State Polytechnic University, Humboldt\n  \n  \n  \n  \n    \n      BA in Psychology —\n      2017\n    \n    California State Polytechnic University, Humboldt"
  }
]