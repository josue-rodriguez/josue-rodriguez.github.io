[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "posts",
    "section": "",
    "text": "Intepreting the confidence interval in front of you\n\n\n\n\n\nI go over the most common interpretation of a confidence interval, and discuss an alternative, potentially more useful one. \n\n\n\n\n\nDec 16, 2024\n\n\nJosue Rodriguez\n\n\n\n\n\n\n\n\n\n\n\n\nConnecting Scalar and Matrix Notation in OLS Regression\n\n\n\n\n\nI go over OLS regression in matrix notation, and show how it connects to the scalar notation of OLS regression. \n\n\n\n\n\nNov 16, 2024\n\n\nJosue Rodriguez\n\n\n\n\n\n\n\n\n\n\n\n\n(simple) Linear regression\n\n\n\n\n\nA “warm up” post covering simple ordinary least squares regression. \n\n\n\n\n\nSep 29, 2024\n\n\nJosue Rodriguez\n\n\n\n\n\n\n\n\n\n\n\n\nModel Selection Bias\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2020\n\n\nJosue Rodriguez\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "josue rodriguez",
    "section": "",
    "text": "Welcome! My name is Josue (hear my name).\nI’m a data scientist at McGraw Hill where I’m helping build personalized learning experiences at scale. Previously, I completed my PhD in Quantitative Psychology at UC Davis where I focused on developing Bayesian statistical methods for social-behavior science."
  },
  {
    "objectID": "posts/2024-11-27-confidence-interval-alt-definition/index.html",
    "href": "posts/2024-11-27-confidence-interval-alt-definition/index.html",
    "title": "Intepreting the confidence interval in front of you",
    "section": "",
    "text": "A confidence interval contains the parameter values that are consistent with your observed sample data.\nFor example, suppose we observe a sample mean \\(\\bar{x} = 100\\) and calculate the \\(95\\%\\) confidence interval for the population mean to be \\([90, 110]\\). This interval suggests that any population mean between \\(90\\) and \\(110\\) could plausibly lead to observing a sample mean of \\(100\\).\nThe main point of this post is that the above interpretation of a confidence interval is a valid one, and that it is more useful than the conventional interpretation. If you’d like to better understand what the above interpretation means and why it’s valid, I invite you to read on."
  },
  {
    "objectID": "posts/2024-11-27-confidence-interval-alt-definition/index.html#set-up",
    "href": "posts/2024-11-27-confidence-interval-alt-definition/index.html#set-up",
    "title": "Intepreting the confidence interval in front of you",
    "section": "Set up",
    "text": "Set up\nSuppose we sample \\(x_1, \\dots, x_n\\) from a normal distribution \\(\\mathcal{N}(\\mu, \\sigma^2)\\), and that we are interested in testing the follow hypotheses about the population mean\n\\[\n\\begin{align}\nH_0{:\\;}\\mu &= \\theta \\\\\n& \\text{vs.} \\\\\nH_1{:\\;} \\mu &\\neq \\theta,\n\\end{align}\n\\]\nfor some \\(\\theta\\)1. We can choose a significance level \\(\\alpha\\) and perform a two-sided \\(t\\)-test to assess this null hypothesis. Recall that our \\(\\alpha\\) level is our error rate – the probability that we reject the null hypothesis when the null hypothesis is true. Canonically, \\(\\theta = 0\\) and \\(\\alpha = 0.05\\).\nWe first find our critical value for this test, \\(t_{crit, \\alpha}\\). We get this value by finding the \\(1 - \\alpha/2\\) quantile of the \\(\\text{Student-}t\\) distribution with \\(n-1\\) degrees of freedom.\nWe can then calculate our observed \\(t\\)-statistic as\n\\[\nt_{obs} = \\frac{\\bar{x} - \\theta}{s/\\sqrt{n}}\n\\]\nwhere \\(\\bar{x}\\) denotes the sample mean and \\(s\\) denotes the sample standard deviation.\nWe reject the null hypothesis that \\(\\mu = \\theta\\) when the \\(p\\)-value is greater than \\(\\alpha\\), or \\(|t_{obs}| \\geq t_{crit, \\alpha}.\\) Conversely, we fail to reject the null hypothesis whenever the \\(p\\)-value is less than \\(\\alpha\\), or \\(|t_{obs}| &lt; t_{crit, \\alpha}\\)."
  },
  {
    "objectID": "posts/2024-11-27-confidence-interval-alt-definition/index.html#footnotes",
    "href": "posts/2024-11-27-confidence-interval-alt-definition/index.html#footnotes",
    "title": "Intepreting the confidence interval in front of you",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’ll focus on hypothesis tests concerning populuation means in this post, but the arguments are applicable to other tests as well, e.g., those concerning mean differences or regression coefficients.↩︎\nJulia Rohrer says the following on her blog, though, and I do think it’s a point well-worth considering\n\nWhat are the downstream consequences of rampant misinterpretation of confidence intervals? As far as I can tell, the whole exercise seems mostly concerned about language. If I say “with a probability of 95%, this interval contains the true parameter”, I commit a faux pas. If I say “here’s my interval; in the long run, 95% of intervals created in this manner contain the true value”, I exhibit technical sophistication. But will any of my downstream inferences look differently? Is any consumer of my findings going to deal with the information differently?\n\n↩︎\nThe \\(p\\)-values are exactly 0.05 for the lower and upper bounds of the interval, and increase to 1 as you move toward the middle of the CI to the sample mean.↩︎"
  },
  {
    "objectID": "posts/2020-6-18-model-selection-bias/model-selection-bias.html",
    "href": "posts/2020-6-18-model-selection-bias/model-selection-bias.html",
    "title": "Model Selection Bias",
    "section": "",
    "text": "Over the last few months, a frequent topic of conversation with my lab mate Donny has been the issue of valid inference following model selection, or model selection bias. This problem has been recognized since at least 1963 and has been written about extensively since then. Some resources I have found both helpful and accessible in understanding model selection bias can be found here, here, and here. However, this issue is still pervasive among social and behavioral scientists 1, so I am writing a short post here in hopes of clarifying the ramifications of drawing inference after selecting a model."
  },
  {
    "objectID": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#why-does-this-occur",
    "href": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#why-does-this-occur",
    "title": "Model Selection Bias",
    "section": "Why does this occur?",
    "text": "Why does this occur?\nAn assumption behind the majority of inferential procedures is that the model is fixed, but model selection makes the model itself random. Inferential procedures typically do not account for this stochastic aspect. Using confidence intervals, let’s take a second to think about why model selection introduces randomness.\nWikipedia defines a 90% confidence interval in terms of sampling by stating\n\nWere this procedure to be repeated on numerous samples, the fraction of calculated confidence intervals (which would differ for each sample) that encompass the true population parameter would tend toward 90%.”\n\nWhat I would like to emphasize here is that that valid confidence intervals have a long run guarantee of covering the true population parameter (approximately) 90 out of 100 times, and this guarantee is based on carrying out the same procedure repeatedly. In this context, procedure refers to an estimation procedure and implies that the statistical model used for inference does not change from sample to sample.\nNow, suppose we have collected some data and we have some candidate set of predictors. We then pick a subset of these predictors according to some model selection procedure and estimate a model with this subset. The end result is an estimation procedure that is conditional on the selected variables.\nIf we do this repeatedly — collect some new data, select a model, and fit it with the selected predictors — there is no guarantee that the estimation procedure will have the same predictors each time 2. It is this randomness introduced by model selection that invalidates the properties of classical inference."
  },
  {
    "objectID": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#coverage",
    "href": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#coverage",
    "title": "Model Selection Bias",
    "section": "Coverage",
    "text": "Coverage\nAfter running the simulations, taking the means of covered_condit and covered_full yields the proportion of times that the credible interval covered the true parameter value when a model was selected and when it was not.\n\n\n\n\n\n\n\n\n\nIt is clear to see from this plot that when a model is not selected, the coverage rate is just about what we would expect — approximately 90 in every 100 credible intervals contained the true value for expected influence. But, when the model was selected prior to computing this interval, the true value was covered much less frequently. Only about 70 in 100 times!"
  },
  {
    "objectID": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#sampling-distribution",
    "href": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#sampling-distribution",
    "title": "Model Selection Bias",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\nRecall that model selection also distorts the sampling distribution of parameters. In the simulations, I kept track of the partial correlation between variables 1 and 6. On each simulation trial, I simply took the mean of the posterior distribution for this parameter. While we would expect to see the means normally distributed around the true value (red line), the distribution is truncated when a model is selected 6. This results in overconfident inferences about the population value. Also notice that parameters estimated after model selection are biased towards large effects. This makes sense as larger effects are more likely to be “significant”.\n\n\n\n\n\n\n\n\n\nThe simulations and plots are simple, but they convey a powerful idea. Model selection distorts inferential properties such as coverage rates and sampling distributions."
  },
  {
    "objectID": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#footnotes",
    "href": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#footnotes",
    "title": "Model Selection Bias",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOther fields as well but I’m in psychology so that is my focus↩︎\nOne could argue that this is not an issue with a consistent model selector or large enough sample sizes, but see Leeb & Pötscher (2005)↩︎\nIf you have any interest in network models at all I highly suggest checking out the BGGM package! It offers a variety of flexible methods for both exploratory and confirmatory analyses plus it now handles ordinal data and VAR models↩︎\nA 90% credible interval can literally be interpreted as a 90% probability that the true parameter value is covered, given the data↩︎\n“frequentist properties of Bayesian methods” is a good google search if you’re bored↩︎\nThere are methods to correct this truncation, but this is still an active area of research↩︎\nTo account for uncertainty in model selection some propose model averaging↩︎\nThere is a growing body of literature on “undoing” model selection bias, but it is not yet a mature body of literature↩︎"
  },
  {
    "objectID": "posts/2024-09-15-simple-linear-regression/index.html",
    "href": "posts/2024-09-15-simple-linear-regression/index.html",
    "title": "(simple) Linear regression",
    "section": "",
    "text": "It’s been a couple years since I’ve taught intro to stats to the incoming grad students in the Psychology department at UC Davis, and it’s probably the only thing I really find myself missing about graduate school. One of my favorite parts of teaching was the fact that I got to revisit the basics of statistics every year. It was a chance to look at the foundations through the lens of whatever I had learned in the past year. I don’t miss writing as much, but I suppose that I miss that too.\nWith that in my mind, I want to start writing again, and I’ll start by writing series of short blog posts covering the basics of statistics with the intentions of brushing up on the basics, and indulging my writing whims. Eventually I hope to get to writing posts as a form of learning and retaining new topics. Until then though…I will consider these “warm ups” – they won’t necessarily be thorough, complete, or accurate.1"
  },
  {
    "objectID": "posts/2024-09-15-simple-linear-regression/index.html#r2",
    "href": "posts/2024-09-15-simple-linear-regression/index.html#r2",
    "title": "(simple) Linear regression",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nPerhaps the most commonly computed measure of model fit for regression is called \\(R^2\\), and it’s used to obtain a sense of how well the variable \\(x\\) “explains” the variation in \\(y\\). Having values in \\([0,1]\\), an \\(R^2 = 1\\) indicates that \\(x\\) completely explains all the variation in \\(y\\) whereas \\(R^2=0\\) indidcates \\(x\\) does not explain any variation in \\(y\\).\nDespite it’s popularity, \\(R^2\\) is often a poor measure of model fit. A couple reasons being that it increases with the number of predictor variables, regardless of whether those variables have any association to \\(y\\), and it can be arbitrarily low in a situation where we have fit the true data-generating model. For more details (and a chuckle), check out section 3.2 of the regression notes here.\n\\[\n\\begin{align}\nR^2 &= 1 - \\frac{\\text{SSR}}{\\text{SST}} \\\\\n\\text{SST} &= \\sum_{i=1}^n \\left(y_i - \\bar y \\right)^2\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/2024-09-15-simple-linear-regression/index.html#mean-squared-error",
    "href": "posts/2024-09-15-simple-linear-regression/index.html#mean-squared-error",
    "title": "(simple) Linear regression",
    "section": "Mean squared error",
    "text": "Mean squared error\nInstead of settling for \\(R^2\\) or some variant of it, we can inspect the mean-squared error (MSE). As the name implies, the MSE tells how large our (squared) residuals are on average. The MSE takes a value of 0 when \\(x\\) perfectly predicts \\(y\\), and it can be arbitrarily large otherwise. It provides a direct measure of how well \\(x\\) predicts \\(y\\).\n\\[\n\\begin{align}\n\\text{MSE} &= \\frac{\\sum_{i=1}^n \\left(\\hat y_i - y_i\\right)^2}{n} \\\\\n\\text{MSE} &= \\frac1n \\sum_{i=1}^n \\hat \\varepsilon_i\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/2024-09-15-simple-linear-regression/index.html#residual-variance",
    "href": "posts/2024-09-15-simple-linear-regression/index.html#residual-variance",
    "title": "(simple) Linear regression",
    "section": "Residual variance",
    "text": "Residual variance\nAnother quantity of interest is the estimated residual variance \\(\\hat \\sigma^2_{\\varepsilon}\\). This also tells us how much we expect a given \\(y_i\\) to deviate from our regression line. It’s almost computed almost exactly as the MSE, but it differs in its denominator. This difference makes it an unbiased estimator of the true residual variance, \\(\\sigma^2\\)\n\\[\n\\begin{align}\n\\hat \\sigma^2_{\\varepsilon} &= \\frac{\\sum_{i=1}^n \\left(\\hat y_i - y_i\\right)^2}{n - p - 1} \\\\\n&=  \\frac{1}{n-2}\\sum_{i=1}^n \\hat \\varepsilon_i .\n\\end{align}\n\\]\nNote that the degrees of freedom account for the number of parameters we estimated. So the degrees of freedom are given by \\(n-p-1\\), where \\(p\\) is the number of predictor variables and the one comes from accounting for the fact that we estimated an intercept.\nThe residual variance is assumed to be constant across all values of \\(x\\). That is \\(E[\\sigma^2_{\\varepsilon} | x] = \\sigma^2_{\\varepsilon}\\). At least for the data that arises from social-behavioral studies, this is often an unrealistic assumption. Two ways we can avoid this assumption by taking a weighted least-squares approach or by directly modeling the variance."
  },
  {
    "objectID": "posts/2024-09-15-simple-linear-regression/index.html#putting-our-coefficients-to-the-test",
    "href": "posts/2024-09-15-simple-linear-regression/index.html#putting-our-coefficients-to-the-test",
    "title": "(simple) Linear regression",
    "section": "Putting our coefficients to the test",
    "text": "Putting our coefficients to the test\nRight right, so we can assess whether our model is doing it’s job (“explaining” the current variation in \\(y\\) or predicting new values in \\(y\\)). BUT, what if we wanted to say something about whether \\(\\beta_1\\) was statistically distinguishable from \\(0\\)? That is, what if we wanted to say that the “effect” of \\(x\\) is non-zero?\nThe coefficient \\(\\beta_1\\) is a random variable, and so we can calculate it’s standard error, and consequently, we can perform a hypothesis test for the following hypotheses\n\\[\n\\begin{aligned}\nH_0: \\beta_1 &= \\beta \\\\\n& \\text{vs.} \\\\\nH_1: \\beta_1 & \\neq \\beta\n\\end{aligned}\n\\]\nUsually, \\(\\beta = 0\\), so we test that the regression coefficient for \\(x\\) is equal to 0, but we could pick any ol’ number.\nThe standard error of \\(\\hat \\beta_1\\) is given by\n\\[\n\\begin{aligned}\n\\text{Var}(\\hat \\beta_1) &= \\frac{\\hat \\sigma^2_{\\varepsilon}}{\\sum_{i=1}^n (x_i - \\bar x)^2} \\\\\n\\text{SE}(\\hat \\beta_1) &= \\sqrt{ \\text{Var}(\\hat \\beta_1) }.\n\\end{aligned}\n\\]\nWith the standard error in hand, we can calculate a \\(t\\)-statistic as follows\n\\[\nt = \\frac{\\hat \\beta - \\beta}{\\text{SE}(\\hat \\beta)} %\\sim \\text{Student-}t\\left(\\nu = n-2\\right)\n\\]\nand compute a \\(p\\)-value by referencing it against a \\(t\\)-distribution with degrees of freedom \\(\\nu = n-2\\)."
  },
  {
    "objectID": "posts/2024-09-15-simple-linear-regression/index.html#footnotes",
    "href": "posts/2024-09-15-simple-linear-regression/index.html#footnotes",
    "title": "(simple) Linear regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’d suggest reading one of Gelman’s books or Cosma Shalizi’s book if that’s what you’re looking for↩︎\nHow many processes can you think of that can truly be described this way? 🤔↩︎"
  },
  {
    "objectID": "posts/2024-11-16-matrix-ols/index.html",
    "href": "posts/2024-11-16-matrix-ols/index.html",
    "title": "Connecting Scalar and Matrix Notation in OLS Regression",
    "section": "",
    "text": "I think my journey in statistics has been unconventional in that much of my statistical training came from social-behavioral stats classes, where the heavier math is often left at the door, and so I missed out on a lot of the intuition and proofs that many statistics students see in their undergraduate courses.\nFor me, this is certainly the case in connecting the estimating equations for the regression parameters (and their variances) that you learn in intro statistics courses to the estimating equation in the matrix form of OLS. I know the scalar equations and I know the matrix equation. But I haven’t seen how it is that they result in the same estimates.\nThis gap in understanding led me to reflect on a statistics professor I had during my master’s program. When it came to derivations with important results, he’d often insist that we “see it once!”. I think what he wanted was for us to know that the methods he was teaching stood on solid ground, and not necessarily for us to memorize each step of the derivation. The idea of “see it once” always struck me as a valuable principle.\nThis post is another opportunity to “see it once”, bridging the scalar and matrix notations of OLS. The derivations below follow Chapter 11 of Cosma Shalizi’s The Truth about Linear Regression, with a few added details for clarity."
  },
  {
    "objectID": "posts/2024-11-16-matrix-ols/index.html#elements-of-hatboldsymbolbeta",
    "href": "posts/2024-11-16-matrix-ols/index.html#elements-of-hatboldsymbolbeta",
    "title": "Connecting Scalar and Matrix Notation in OLS Regression",
    "section": "Elements of \\(\\hat{\\boldsymbol{\\beta}}\\)",
    "text": "Elements of \\(\\hat{\\boldsymbol{\\beta}}\\)\nOkay so we’ve seen that the estimating equation \\(\\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}' \\mathbf{y}\\) results in a vector of parameter estimates that minimizes the RSS, but let’s make sure that the elements in \\(\\hat{\\bm{\\beta}}\\) indeed correspond to their scalar counterparts.\nSpecifically, let’s verify that\n\\[\n\\hat{\\bm{\\beta}} =\n\\begin{bmatrix}\\hat\\beta_0 \\\\ \\hat\\beta_1\\end{bmatrix} =\n\\begin{bmatrix}\n\\bar y -  \\hat \\beta_1 \\bar x \\\\\n\\frac{{\\text{Cov}(x, y)}}{\\hat \\sigma^2_x}\n\\end{bmatrix}.\n\\]\nAs a starting point, we’ll introduce a normalizing factor \\(n^{-1}\\) to the estimating equation1 so that\n\\[\n\\hat{\\bm{\\beta}} =\n\\left(\n   n^{-1}\\mathbf{x}'\\mathbf{x}\n\\right)^{-1} n^{-1}\\mathbf{x}'\\mathbf{y}.\n\\]\n\nWe’ll compute this product in parts, beginning with the terms inside the parentheses. I’ll note here that in what follows, estimates of variances are defined as their maximum likelihood estimates.\n\\[\n\\begin{align}\nn^{-1}\\mathbf{x}'\\mathbf{x} &=\n\\frac1n\n\\begin{bmatrix}\n1 & \\dots & 1 \\\\\nx_1 & \\dots & x_n\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & x_1 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n\n\\end{bmatrix} \\\\\n% ---------\n&= \\frac1n \\begin{bmatrix}\nn & \\sum x_i \\\\\n\\sum x_i & \\sum x^2_i\n\\end{bmatrix} \\\\\n% ---------\n&= \\begin{bmatrix}\n1 & \\bar{x}\\\\\n\\bar{x}& \\bar{x^2}\n\\end{bmatrix}\n\\end{align}\n\\]\nLet’s not forget that we need to find the inverse of the resulting matrix 2\n\\[\n\\begin{align}\n\\left(n^{-1}\\mathbf{x}'\\mathbf{x}\\right)^{-1} &=\n  \\begin{bmatrix}\n    1 & \\sum \\bar{x}\\\\\n    \\bar{x}& \\sum \\bar{x^2}\n  \\end{bmatrix}^{-1} \\\\\n% ---------\n&= \\frac{1}{\\bar{x^2} - \\bar{x}^2} \\begin{bmatrix}\n\\bar{x^2} & -\\bar{x}\\\\\n-\\bar{x} & 1  \n\\end{bmatrix}\\\\\n% ---------\n&= \\frac{1}{\\hat\\sigma^2_x} \\begin{bmatrix}\n\\bar{x^2} & -\\bar{x}\\\\\n-\\bar{x} & 1  \n\\end{bmatrix}.\n\\end{align}\n\\]\n\nWe get \\(\\hat\\sigma^2_x\\) because \\(\\bar{x^2} - \\bar{x}^2\\) estimates \\(\\mathbb{V}[x] = \\mathbb{E}[x^2] - \\mathbb{E}[x]^2\\).\n\n\nNext, we’ll work through the term outside the parentheses\n\\[\n\\begin{align}\nn^{-1}\\mathbf{x}'\\mathbf{y} &=\n\\frac1n \\begin{bmatrix}\n  1 & \\dots & 1 \\\\\n  x_1 & \\dots & x_n\n\\end{bmatrix}\n\\begin{bmatrix}\n  y_i \\\\\n  \\vdots \\\\\n  y_n\n\\end{bmatrix}\\\\\n% ---------\n&= \\frac1n \\begin{bmatrix}\n  \\sum y_i \\\\\n  \\sum x_i y_i\n\\end{bmatrix} \\\\\n% ---------\n&= \\begin{bmatrix}\n\\bar{y} \\\\\n\\overline{xy}\n\\end{bmatrix}.\n\\end{align}\n\\]\n\nStitching it all together results in the following vector\n\\[\n\\begin{align}\n\\hat{\\bm{\\beta}} &=\n\\left(\n  n^{-1}\\mathbf{x}'\\mathbf{x}\n\\right)^{-1} n^{-1}\\mathbf{x}'\\mathbf{y} \\\\\n% ---------\n&= \\frac{1}{\\hat\\sigma^2_x}\n\\begin{bmatrix}\n  \\bar{x^2} & -\\bar{x}\\\\\n  -\\bar{x} & 1  \n\\end{bmatrix}\n\\begin{bmatrix}\n  \\bar{y} \\\\\n  \\overline{xy}\n\\end{bmatrix} \\\\\n% ---------\n&= \\frac{1}{\\hat\\sigma^2_x}\n\\begin{bmatrix}\n  \\bar{x^2}\\bar{y} - \\bar{x} \\overline{xy} \\\\\n  - \\bar{x} \\bar{y} + \\overline{xy}\n\\end{bmatrix}.\n% ---------\n\\end{align}\n\\]\nWe’ve made good progress up to this point, but it still feels unclear how the elements of this vector correspond to the scalar equations for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\). To continue, we’ll have re-express the elements of the vector by taking advantage of the following definitions\n\n\\(\\hat{\\sigma^2_x} = \\bar{x^2} - \\bar{x}^2\\)\n\\(\\hat{\\text{Cov}(x, y)} = \\overline{xy} - \\bar{x}\\bar{y}\\).\n\nDoing so lets us re-express the vector as follows \\[\n\\begin{align}\n&= \\frac{1}{\\hat\\sigma^2_x}\n\\begin{bmatrix}\n  \\left(\\hat\\sigma^2_x + \\bar{x}^2\\right)\\bar{y} - \\bar{x}\\left(\\hat{\\text{Cov}(x,y)} + \\bar{x} \\bar{y}\\right) \\\\\n  \\hat{\\text{Cov}(x,y)}\n\\end{bmatrix} \\\\\n% ---------\n&= \\frac{1}{\\hat\\sigma^2_x}\n\\begin{bmatrix}\n  \\hat\\sigma^2_x\\bar{y} + \\bar{x}^2\\bar{y} - \\bar{x}\\hat{\\text{Cov}(x,y)} - \\bar{x}^2 \\bar{y} \\\\\n  \\hat{\\text{Cov}(x,y)}\n\\end{bmatrix}.\n% ---------\n\\end{align}\n\\]\nOut last task here is distributing \\(\\frac{1}{\\hat\\sigma^2_x}\\) and simplifying\n\\[\n\\begin{align}\n&=\n\\begin{bmatrix}\n  \\bar{y} - \\frac{\\hat{\\text{Cov}(x,y)}}{\\hat\\sigma^2_x}\\bar{x} \\\\\n  \\frac{\\hat{\\text{Cov}(x,y)}}{\\hat\\sigma^2_x}\n\\end{bmatrix} \\\\\n% ---------\n&=\n\\begin{bmatrix}\n  \\bar{y} - \\hat\\beta_1\\bar{x} \\\\\n  \\hat \\beta_1\n\\end{bmatrix}.\n\\end{align}\n\\]\nAnd there it is. Although it was a bit tedious, we can see that the elements in \\(\\hat{\\bm{\\beta}}\\) indeed correspond to the scalar equations for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\)."
  },
  {
    "objectID": "posts/2024-11-16-matrix-ols/index.html#elements-of-the-variance-covariance-matrix",
    "href": "posts/2024-11-16-matrix-ols/index.html#elements-of-the-variance-covariance-matrix",
    "title": "Connecting Scalar and Matrix Notation in OLS Regression",
    "section": "Elements of the variance-covariance matrix",
    "text": "Elements of the variance-covariance matrix\nAs we did with the parameter estimates, we want to make sure that the matrix representation of parameter variances correspond to their scalar counterparts.\nTo verify, we’ll again start by adding in a normalizing term of \\(n^{-1}\\) so that\n\\[\n\\begin{align}\n{\\bm{\\Sigma}} &= n^{-1}\\sigma^2_{\\epsilon}  \\left(n^{-1}\\mathbf{x}'\\mathbf{x}\\right)^{-1} \\\\\n% ---------\n&= \\frac{\\sigma^2_{\\epsilon}}{n}  \\left(\\frac{1}{\\hat\\sigma^2_x} \\begin{bmatrix}\n\\bar{x^2} & -\\bar{x}\\\\\n-\\bar{x} & 1  \n\\end{bmatrix}\n\\right).\n\\end{align}\n\\]\nThe variance for \\(\\hat\\beta_0\\) corresponds to the first diagonal element of \\({\\bm{\\Sigma}}\\). Multiplying the relevant terms gets us\n\\[\n\\begin{align}\n\\mathbb{V}[\\hat\\beta_0] &= \\frac{\\sigma^2_{\\epsilon}}{n} \\cdot \\frac{1}{\\hat\\sigma^2_x} \\cdot \\bar{x^2} \\\\\n% ---------\n&= \\frac{\\sigma^2_{\\epsilon}\\bar{x^2} }{n \\hat\\sigma^2_x} \\\\\n% ---------\n&= \\frac{\\sigma^2_{\\epsilon}( \\bar{x}^2  + \\hat\\sigma^2_x )}{n \\hat\\sigma^2_x} \\\\\n% ---------\n&= \\frac{\\sigma^2_{\\epsilon}\\bar{x}^2  + \\sigma^2_{\\epsilon}\\hat\\sigma^2_x}{n \\hat\\sigma^2_x} \\\\\n% ---------\n&= \\frac{\\sigma^2_{\\epsilon}\\bar{x}^2}{n \\hat\\sigma^2_x} + \\frac{\\sigma^2_{\\epsilon}\\hat\\sigma^2_x}{n \\hat\\sigma^2_x} \\\\\n% ---------\n&= \\frac{\\sigma^2_{\\epsilon}}{n} \\left(\n  \\frac{\\bar{x}^2}{\\sigma^2_x} + 1\n\\right).\n\\end{align}\n\\]\nThe variance for \\(\\hat\\beta_1\\) corresponds to the second diagonal element of \\({\\bm{\\Sigma}}\\), and again multiplying the relevant terms\n\\[\n\\begin{align}\n\\mathbb{V}[\\hat\\beta_1] &= \\frac{\\sigma^2_{\\epsilon}}{n} \\cdot \\frac{1}{\\hat\\sigma^2_x} \\cdot 1 \\\\\n% ---------\n&= \\frac{\\sigma^2_{\\epsilon}}{n} \\cdot \\frac{1}{\\frac{\\sum (x_i - \\bar{x})^2}{n}}\\\\\n% ---------\n&= \\frac{\\sigma^2_{\\epsilon}}{\\frac{n\\sum (x_i - \\bar{x})^2}{n}}\\\\\n% ---------\n&= \\frac{\\sigma^2_{\\epsilon}}{\\sum_{i=1}^n\\left(x_i - \\bar{x}\\right)^2}.\n\\end{align}\n\\]\nWith that, we’ve shown that the matrix representation of the parameter variances also match their scalar representations."
  },
  {
    "objectID": "posts/2024-11-16-matrix-ols/index.html#footnotes",
    "href": "posts/2024-11-16-matrix-ols/index.html#footnotes",
    "title": "Connecting Scalar and Matrix Notation in OLS Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis will be handy in letting us define the elements in the relevant vectors and matrices as means and variances.↩︎\nThe inverse of a \\(2 \\times 2\\) matrix is given by \\(\\left[\\begin{smallmatrix}a & b \\\\ c & d\\end{smallmatrix}\\right]^{-1} = \\frac{1}{ad-bc}\\left[\\begin{smallmatrix}d & -b \\\\ -c & a\\end{smallmatrix}\\right]\\).↩︎\nThis stems from the fact that we defined \\(\\hat{\\bm{\\beta}} = \\bm{\\beta} + \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}'\\bm{\\epsilon}\\) and the normality of \\(\\bm{\\epsilon}\\) is preserved under affine transformations.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "Hi! My name is Josue (hear my name).\nI’m a data scientist at McGraw Hill where I’m helping build personalized learning experiences at scale. I previously earned a PhD in Quantitative Psychology at UC Davis, where I focused on developing Bayesian statistical models for social-behavior science."
  },
  {
    "objectID": "about.html#work",
    "href": "about.html#work",
    "title": "about",
    "section": "",
    "text": "Hi! My name is Josue (hear my name).\nI’m a data scientist at McGraw Hill where I’m helping build personalized learning experiences at scale. I previously earned a PhD in Quantitative Psychology at UC Davis, where I focused on developing Bayesian statistical models for social-behavior science."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "about",
    "section": " education",
    "text": "education\n\n\n\n  \n    \n      PhD in Quantitative Psychology —\n      2023\n    \n    University of California, Davis\n  \n  \n  \n    \n      MA in Cognitive Psychology —\n      2019\n    \n    Minor in Statistics\n    California State Polytechnic University, Humboldt\n  \n  \n  \n  \n    \n      BA in Psychology —\n      2017\n    \n    California State Polytechnic University, Humboldt"
  },
  {
    "objectID": "posts/2024-11-27-confidence-interval-alt-definition/index.html#problem-set-up",
    "href": "posts/2024-11-27-confidence-interval-alt-definition/index.html#problem-set-up",
    "title": "Intepreting the confidence interval in front of you",
    "section": "Problem Set-up",
    "text": "Problem Set-up\nSuppose we sample \\(x_1, \\dots, x_n\\) from a \\(\\mathcal{N}(\\mu, \\sigma^2)\\) distribution, and that we are interested in testing the follow hypotheses about the population mean\n\\[\n\\begin{align}\nH_0{:\\;}\\mu &= \\theta \\\\\n& \\text{vs.} \\\\\nH_1{:\\;} \\mu &\\neq \\theta,\n\\end{align}\n\\]\nfor some \\(\\theta\\)1. We can choose a significance level \\(\\alpha\\) and perform a two-sided \\(t\\)-test to assess this null hypothesis. Recall that our \\(\\alpha\\) level is our error rate – the probability that we reject the null hypothesis when the null hypothesis is true. Canonically, \\(\\theta = 0\\) and \\(\\alpha = 0.05\\).\nWe first find our critical value for this test, \\(t_{crit, \\alpha}\\). We get this value by finding the \\(1 - \\alpha/2\\) quantile of the \\(\\text{Student-}t\\) distribution with \\(n-1\\) degrees of freedom.\nWe can then calculate our observed \\(t\\)-statistic as\n\\[\nt_{obs} = \\frac{\\bar{x} - \\theta}{s/\\sqrt{n}}\n\\]\nwhere \\(\\bar{x}\\) denotes the sample mean and \\(s\\) denotes the sample standard deviation.\nWe reject the null hypothesis that \\(\\mu = \\theta\\) when the \\(p\\)-value is greater than \\(\\alpha\\), or \\(|t_{obs}| \\geq t_{crit, \\alpha}.\\) Conversely, we fail to reject the null hypothesis whenever the \\(p\\)-value is less than \\(\\alpha\\), or \\(|t_{obs}| &lt; t_{crit, \\alpha}\\)."
  }
]