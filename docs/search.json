[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "posts",
    "section": "",
    "text": "Connecting Scalar and Matrix Notation in OLS\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2024\n\n\nJosue Rodriguez\n\n\n\n\n\n\n\n\n\n\n\n\n(simple) Linear regression\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2024\n\n\nJosue Rodriguez\n\n\n\n\n\n\n\n\n\n\n\n\nModel Selection Bias\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2020\n\n\nJosue Rodriguez\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "josue rodriguez",
    "section": "",
    "text": "Welcome! My name is Josue (pronounced ho-sway).\nI‚Äôm a data scientist at McGraw Hill where I‚Äôm helping build personalized learning experiences at scale. Previously, I completed my PhD in Quantitative Psychology at UC Davis where I focused on developing Bayesian statistical models for social-behavior science."
  },
  {
    "objectID": "posts/2024-09-15-simple-linear-regression/index.html",
    "href": "posts/2024-09-15-simple-linear-regression/index.html",
    "title": "(simple) Linear regression",
    "section": "",
    "text": "It‚Äôs been a couple years since I‚Äôve taught intro to stats to the incoming grad students in the Psychology department at UC Davis, and it‚Äôs probably the only thing I really find myself missing about graduate school. One of my favorite parts of teaching was the fact that I got to revisit the basics of statistics every year. It was a chance to look at the foundations through the lens of whatever I had learned in the past year. I don‚Äôt miss writing as much, but I suppose that I miss that too.\nWith that in my mind, I want to start writing again, and I‚Äôll start by writing series of short blog posts covering the basics of statistics with the intentions of brushing up on the basics, and indulging my writing whims. Eventually I hope to get to writing posts as a form of learning and retaining new topics. Until then though‚Ä¶I will consider these ‚Äúwarm ups‚Äù ‚Äì they won‚Äôt necessarily be thorough, complete, or accurate.1"
  },
  {
    "objectID": "posts/2024-09-15-simple-linear-regression/index.html#r2",
    "href": "posts/2024-09-15-simple-linear-regression/index.html#r2",
    "title": "(simple) Linear regression",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nPerhaps the most commonly computed measure of model fit for regression is called \\(R^2\\), and it‚Äôs used to obtain a sense of how well the variable \\(x\\) ‚Äúexplains‚Äù the variation in \\(y\\). Having values in \\([0,1]\\), an \\(R^2 = 1\\) indicates that \\(x\\) completely explains all the variation in \\(y\\) whereas \\(R^2=0\\) indidcates \\(x\\) does not explain any variation in \\(y\\).\nDespite it‚Äôs popularity, \\(R^2\\) is often a poor measure of model fit. A couple reasons being that it increases with the number of predictor variables, regardless of whether those variables have any association to \\(y\\), and it can be arbitrarily low in a situation where we have fit the true data-generating model. For more details (and a chuckle), check out section 3.2 of the regression notes here.\n\\[\n\\begin{align}\nR^2 &= 1 - \\frac{\\text{SSR}}{\\text{SST}} \\\\\n\\text{SST} &= \\sum_{i=1}^n \\left(y_i - \\bar y \\right)^2\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/2024-09-15-simple-linear-regression/index.html#mean-squared-error",
    "href": "posts/2024-09-15-simple-linear-regression/index.html#mean-squared-error",
    "title": "(simple) Linear regression",
    "section": "Mean squared error",
    "text": "Mean squared error\nInstead of settling for \\(R^2\\) or some variant of it, we can inspect the mean-squared error (MSE). As the name implies, the MSE tells how large our (squared) residuals are on average. The MSE takes a value of 0 when \\(x\\) perfectly predicts \\(y\\), and it can be arbitrarily large otherwise. It provides a direct measure of how well \\(x\\) predicts \\(y\\).\n\\[\n\\begin{align}\n\\text{MSE} &= \\frac{\\sum_{i=1}^n \\left(\\hat y_i - y_i\\right)^2}{n} \\\\\n\\text{MSE} &= \\frac1n \\sum_{i=1}^n \\hat \\varepsilon_i\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/2024-09-15-simple-linear-regression/index.html#residual-variance",
    "href": "posts/2024-09-15-simple-linear-regression/index.html#residual-variance",
    "title": "(simple) Linear regression",
    "section": "Residual variance",
    "text": "Residual variance\nAnother quantity of interest is the estimated residual variance \\(\\hat \\sigma^2_{\\varepsilon}\\). This also tells us how much we expect a given \\(y_i\\) to deviate from our regression line. It‚Äôs almost computed almost exactly as the MSE, but it differs in its denominator. This difference makes it an unbiased estimator of the true residual variance, \\(\\sigma^2\\)\n\\[\n\\begin{align}\n\\hat \\sigma^2_{\\varepsilon} &= \\frac{\\sum_{i=1}^n \\left(\\hat y_i - y_i\\right)^2}{n - p - 1} \\\\\n&=  \\frac{1}{n-2}\\sum_{i=1}^n \\hat \\varepsilon_i .\n\\end{align}\n\\]\nNote that the degrees of freedom account for the number of parameters we estimated. So the degrees of freedom are given by \\(n-p-1\\), where \\(p\\) is the number of predictor variables and the one comes from accounting for the fact that we estimated an intercept.\nThe residual variance is assumed to be constant across all values of \\(x\\). That is \\(E[\\sigma^2_{\\varepsilon} | x] = \\sigma^2_{\\varepsilon}\\). At least for the data that arises from social-behavioral studies, this is often an unrealistic assumption. Two ways we can avoid this assumption by taking a weighted least-squares approach or by directly modeling the variance."
  },
  {
    "objectID": "posts/2024-09-15-simple-linear-regression/index.html#putting-our-coefficients-to-the-test",
    "href": "posts/2024-09-15-simple-linear-regression/index.html#putting-our-coefficients-to-the-test",
    "title": "(simple) Linear regression",
    "section": "Putting our coefficients to the test",
    "text": "Putting our coefficients to the test\nRight right, so we can assess whether our model is doing it‚Äôs job (‚Äúexplaining‚Äù the current variation in \\(y\\) or predicting new values in \\(y\\)). BUT, what if we wanted to say something about whether \\(\\beta_1\\) was statistically distinguishable from \\(0\\)? That is, what if we wanted to say that the ‚Äúeffect‚Äù of \\(x\\) is non-zero?\nThe coefficient \\(\\beta_1\\) is a random variable, and so we can calculate it‚Äôs standard error, and consequently, we can perform a hypothesis test for the following hypotheses\n\\[\n\\begin{aligned}\nH_0: \\beta_1 &= \\beta \\\\\n& \\text{vs.} \\\\\nH_1: \\beta_1 & \\neq \\beta\n\\end{aligned}\n\\]\nUsually, \\(\\beta = 0\\), so we test that the regression coefficient for \\(x\\) is equal to 0, but we could pick any ol‚Äô number.\nThe standard error of \\(\\hat \\beta_1\\) is given by\n\\[\n\\begin{aligned}\n\\text{Var}(\\hat \\beta_1) &= \\frac{\\hat \\sigma^2_{\\varepsilon}}{\\sum_{i=1}^n (x_i - \\bar x)^2} \\\\\n\\text{SE}(\\hat \\beta_1) &= \\sqrt{ \\text{Var}(\\hat \\beta_1) }.\n\\end{aligned}\n\\]\nWith the standard error in hand, we can calculate a \\(t\\)-statistic as follows\n\\[\nt = \\frac{\\hat \\beta - \\beta}{\\text{SE}(\\hat \\beta)} %\\sim \\text{Student-}t\\left(\\nu = n-2\\right)\n\\]\nand compute a \\(p\\)-value by referencing it against a \\(t\\)-distribution with degrees of freedom \\(\\nu = n-2\\)."
  },
  {
    "objectID": "posts/2024-09-15-simple-linear-regression/index.html#footnotes",
    "href": "posts/2024-09-15-simple-linear-regression/index.html#footnotes",
    "title": "(simple) Linear regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI‚Äôd suggest reading one of Gelman‚Äôs books or Cosma Shalizi‚Äôs book if that‚Äôs what you‚Äôre looking for‚Ü©Ô∏é\nHow many processes can you think of that can truly be described this way? ü§î‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2020-6-18-model-selection-bias/model-selection-bias.html",
    "href": "posts/2020-6-18-model-selection-bias/model-selection-bias.html",
    "title": "Model Selection Bias",
    "section": "",
    "text": "Over the last few months, a frequent topic of conversation with my lab mate Donny has been the issue of valid inference following model selection, or model selection bias. This problem has been recognized since at least 1963 and has been written about extensively since then. Some resources I have found both helpful and accessible in understanding model selection bias can be found here, here, and here. However, this issue is still pervasive among social and behavioral scientists 1, so I am writing a short post here in hopes of clarifying the ramifications of drawing inference after selecting a model."
  },
  {
    "objectID": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#why-does-this-occur",
    "href": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#why-does-this-occur",
    "title": "Model Selection Bias",
    "section": "Why does this occur?",
    "text": "Why does this occur?\nAn assumption behind the majority of inferential procedures is that the model is fixed, but model selection makes the model itself random. Inferential procedures typically do not account for this stochastic aspect. Using confidence intervals, let‚Äôs take a second to think about why model selection introduces randomness.\nWikipedia defines a 90% confidence interval in terms of sampling by stating\n\nWere this procedure to be repeated on numerous samples, the fraction of calculated confidence intervals (which would differ for each sample) that encompass the true population parameter would tend toward 90%.‚Äù\n\nWhat I would like to emphasize here is that that valid confidence intervals have a long run guarantee of covering the true population parameter (approximately) 90 out of 100 times, and this guarantee is based on carrying out the same procedure repeatedly. In this context, procedure refers to an estimation procedure and implies that the statistical model used for inference does not change from sample to sample.\nNow, suppose we have collected some data and we have some candidate set of predictors. We then pick a subset of these predictors according to some model selection procedure and estimate a model with this subset. The end result is an estimation procedure that is conditional on the selected variables.\nIf we do this repeatedly ‚Äî collect some new data, select a model, and fit it with the selected predictors ‚Äî there is no guarantee that the estimation procedure will have the same predictors each time 2. It is this randomness introduced by model selection that invalidates the properties of classical inference."
  },
  {
    "objectID": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#coverage",
    "href": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#coverage",
    "title": "Model Selection Bias",
    "section": "Coverage",
    "text": "Coverage\nAfter running the simulations, taking the means of covered_condit and covered_full yields the proportion of times that the credible interval covered the true parameter value when a model was selected and when it was not.\n\n\n\n\n\n\n\n\n\nIt is clear to see from this plot that when a model is not selected, the coverage rate is just about what we would expect ‚Äî approximately 90 in every 100 credible intervals contained the true value for expected influence. But, when the model was selected prior to computing this interval, the true value was covered much less frequently. Only about 70 in 100 times!"
  },
  {
    "objectID": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#sampling-distribution",
    "href": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#sampling-distribution",
    "title": "Model Selection Bias",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\nRecall that model selection also distorts the sampling distribution of parameters. In the simulations, I kept track of the partial correlation between variables 1 and 6. On each simulation trial, I simply took the mean of the posterior distribution for this parameter. While we would expect to see the means normally distributed around the true value (red line), the distribution is truncated when a model is selected 6. This results in overconfident inferences about the population value. Also notice that parameters estimated after model selection are biased towards large effects. This makes sense as larger effects are more likely to be ‚Äúsignificant‚Äù.\n\n\n\n\n\n\n\n\n\nThe simulations and plots are simple, but they convey a powerful idea. Model selection distorts inferential properties such as coverage rates and sampling distributions."
  },
  {
    "objectID": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#footnotes",
    "href": "posts/2020-6-18-model-selection-bias/model-selection-bias.html#footnotes",
    "title": "Model Selection Bias",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOther fields as well but I‚Äôm in psychology so that is my focus‚Ü©Ô∏é\nOne could argue that this is not an issue with a consistent model selector or large enough sample sizes, but see Leeb & P√∂tscher (2005)‚Ü©Ô∏é\nIf you have any interest in network models at all I highly suggest checking out the BGGM package! It offers a variety of flexible methods for both exploratory and confirmatory analyses plus it now handles ordinal data and VAR models‚Ü©Ô∏é\nA 90% credible interval can literally be interpreted as a 90% probability that the true parameter value is covered, given the data‚Ü©Ô∏é\n‚Äúfrequentist properties of Bayesian methods‚Äù is a good google search if you‚Äôre bored‚Ü©Ô∏é\nThere are methods to correct this truncation, but this is still an active area of research‚Ü©Ô∏é\nTo account for uncertainty in model selection some propose model averaging‚Ü©Ô∏é\nThere is a growing body of literature on ‚Äúundoing‚Äù model selection bias, but it is not yet a mature body of literature‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2024-11-16-matrix-ols/index.html",
    "href": "posts/2024-11-16-matrix-ols/index.html",
    "title": "Connecting Scalar and Matrix Notation in OLS",
    "section": "",
    "text": "I don‚Äôt know. Maybe my journey in statistics has been nontraditional in that much of my statistical training came from social-behavioral stats classes, where the math-y bits of statistics are de-emphasized, and so I missed out on a lot of the intuition and proofs that many statistics students see in their undergraduate courses.\nFor me, this is certainly the case in connecting the estimating equations for the regression parameters (and their variances) that you learn in intro statistics courses to the estimating equation in the matrix form of OLS. This post is an attempt at making that connection clearer for myself.\nThe derivations below largely come from Chapter 11 in Cosma Shalizi‚Äôs The Truth about Linear Regression. Where I could, I filled in some details."
  },
  {
    "objectID": "posts/2024-11-16-matrix-ols/index.html#elements-of-hatboldsymbolbeta",
    "href": "posts/2024-11-16-matrix-ols/index.html#elements-of-hatboldsymbolbeta",
    "title": "Connecting Scalar and Matrix Notation in OLS",
    "section": "Elements of \\(\\hat{\\boldsymbol{\\beta}}\\)",
    "text": "Elements of \\(\\hat{\\boldsymbol{\\beta}}\\)\nOur next step is to make sure that the elements in \\(\\hat{\\bm{\\beta}}\\) correspond to their scalar counterparts.\nThat is, we want to confirm that\n\\[\n\\hat{\\bm{\\beta}} =\n\\begin{bmatrix}\\hat\\beta_0 \\\\ \\hat\\beta_1\\end{bmatrix} =\n\\begin{bmatrix}\n\\bar y -  \\hat \\beta_1 \\bar x \\\\\n\\frac{{\\text{Cov}[x, y]}}{\\hat \\sigma^2_x}\n\\end{bmatrix}.\n\\]\nAs a starting point, we‚Äôll introduce a normalizing factor \\(n^{-1}\\) to the estimating equation1 so that\n\\[\n\\hat{\\bm{\\beta}} =\n\\left(\n   n^{-1}\\mathbf{x}'\\mathbf{x}\n\\right)^{-1} n^{-1}\\mathbf{x}'\\mathbf{y}.\n\\]\n\nWe‚Äôll then compute this product in parts, beginning with the terms inside the parentheses. I‚Äôll note here that in what follows, estimates of variances are defined as their maximum likelihood estimates.\n\\[\n\\begin{align}\nn^{-1}\\mathbf{x}'\\mathbf{x} &=\n\\frac1n\n\\begin{bmatrix}\n1 & \\dots & 1 \\\\\nx_1 & \\dots & x_n\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & x_1 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n\n\\end{bmatrix} \\\\\n% ---------\n&= \\frac1n \\begin{bmatrix}\nn & \\sum x_i \\\\\n\\sum x_i & \\sum x^2_i\n\\end{bmatrix} \\\\\n% ---------\n&= \\begin{bmatrix}\n1 & \\bar{x}\\\\\n\\bar{x}& \\bar{x^2}\n\\end{bmatrix}\n\\end{align}\n\\]\nLet‚Äôs not forget that we need to find the inverse of the resulting matrix 2\n\\[\n\\begin{align}\n\\left(n^{-1}\\mathbf{x}'\\mathbf{x}\\right)^{-1} &=\n  \\begin{bmatrix}\n    1 & \\sum \\bar{x}\\\\\n    \\bar{x}& \\sum \\bar{x^2}\n  \\end{bmatrix}^{-1} \\\\\n% ---------\n&= \\frac{1}{\\bar{x^2} - \\bar{x}^2} \\begin{bmatrix}\n\\bar{x^2} & -\\bar{x}\\\\\n-\\bar{x} & 1  \n\\end{bmatrix}\\\\\n% ---------\n&= \\frac{1}{\\hat\\sigma^2_x} \\begin{bmatrix}\n\\bar{x^2} & -\\bar{x}\\\\\n-\\bar{x} & 1  \n\\end{bmatrix}.\n\\end{align}\n\\]\n\nWe get \\(\\hat\\sigma^2_x\\) because \\(\\bar{x^2} - \\bar{x}^2\\) estimates \\(\\text{Var}[x] = \\mathbb{E}[x^2] - \\mathbb{E}[x]^2\\).\n\n\nNext, we‚Äôll work through the term outside the parentheses\n\\[\n\\begin{align}\nn^{-1}\\mathbf{x}'\\mathbf{y} &=\n\\frac1n \\begin{bmatrix}\n  1 & \\dots & 1 \\\\\n  x_1 & \\dots & x_n\n\\end{bmatrix}\n\\begin{bmatrix}\n  y_i \\\\\n  \\vdots \\\\\n  y_n\n\\end{bmatrix}\\\\\n% ---------\n&= \\frac1n \\begin{bmatrix}\n  \\sum y_i \\\\\n  \\sum x_i y_i\n\\end{bmatrix} \\\\\n% ---------\n&= \\begin{bmatrix}\n\\bar{y} \\\\\n\\overline{xy}\n\\end{bmatrix}\n\\end{align}\n\\]\n\nStitching it all together we see that\n\\[\n\\begin{align}\n\\hat{\\bm{\\beta}} &=\n\\left(\n  n^{-1}\\mathbf{x}'\\mathbf{x}\n\\right)^{-1} n^{-1}\\mathbf{x}'\\mathbf{y} \\\\\n% ---------\n&= \\frac{1}{\\hat\\sigma^2_x}\n\\begin{bmatrix}\n  \\bar{x^2} & -\\bar{x}\\\\\n  -\\bar{x} & 1  \n\\end{bmatrix}\n\\begin{bmatrix}\n  \\bar{y} \\\\\n  \\overline{xy}\n\\end{bmatrix} \\\\\n% ---------\n&= \\frac{1}{\\hat\\sigma^2_x}\n\\begin{bmatrix}\n  \\bar{x^2}\\bar{y} - \\bar{x} \\overline{xy} \\\\\n  - \\bar{x} \\bar{y} + \\overline{xy}\n\\end{bmatrix}.\n% ---------\n\\end{align}\n\\]\nAt this point, it still feels unclear how the elements of this vector correspond to scalar equations for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\), even though we have seen that it‚Äôs elements minimize the RSS. To continue, we‚Äôll have re-express the elements of the vector by taking advantage of the following definitions\n\n\\(\\hat{\\sigma^2_x} = \\bar{x^2} - \\bar{x}^2\\)\n\\(\\hat{\\text{Cov}[x, y]} = \\overline{xy} - \\bar{x}\\bar{y}\\)\n\nDoing so let‚Äôs us re-write the vector as follows \\[\n\\begin{align}\n&= \\frac{1}{\\hat\\sigma^2_x}\n\\begin{bmatrix}\n  \\left(\\hat\\sigma^2_x + \\bar{x}^2\\right)\\bar{y} - \\bar{x}\\left(\\hat{\\text{Cov}[x,y]} + \\bar{x} \\bar{y}\\right) \\\\\n  \\hat{\\text{Cov}[x,y]}\n\\end{bmatrix} \\\\\n% ---------\n&= \\frac{1}{\\hat\\sigma^2_x}\n\\begin{bmatrix}\n  \\hat\\sigma^2_x\\bar{y} + \\bar{x}^2\\bar{y} - \\bar{x}\\hat{\\text{Cov}[x,y]} - \\bar{x}^2 \\bar{y} \\\\\n  \\hat{\\text{Cov}[x,y]}\n\\end{bmatrix}.\n% ---------\n\\end{align}\n\\]\nOut last step here is distributing \\(\\frac{1}{\\hat\\sigma^2_x}\\)\n\\[\n\\begin{align}\n&=\n\\begin{bmatrix}\n  \\bar{y} - \\frac{\\hat{\\text{Cov}[x,y]}}{\\hat\\sigma^2_x}\\bar{x} \\\\\n  \\frac{\\hat{\\text{Cov}[x,y]}}{\\hat\\sigma^2_x}\n\\end{bmatrix} \\\\\n% ---------\n&=\n\\begin{bmatrix}\n  \\bar{y} - \\hat\\beta_1\\bar{x} \\\\\n  \\hat \\beta_1\n\\end{bmatrix}.\n\\end{align}\n\\]\nAnd there it is. Although it was a bit tedious, we can see that the elements in \\(\\hat{\\bm{\\beta}}\\) indeed correspond to the scalar equations for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\)."
  },
  {
    "objectID": "posts/2024-11-16-matrix-ols/index.html#elements-of-the-variance-covariance-matrix",
    "href": "posts/2024-11-16-matrix-ols/index.html#elements-of-the-variance-covariance-matrix",
    "title": "Connecting Scalar and Matrix Notation in OLS",
    "section": "Elements of the variance-covariance matrix",
    "text": "Elements of the variance-covariance matrix\nAs we did with the parameter estimates, we want to make sure that our matrix representation of the parameter variances correspond to their scalar counterparts.\nTo find out, we‚Äôll again start by adding in a normalizing term of \\(n^{-1}\\) so that\n\\[\n\\begin{align}\n{\\bm{\\Sigma}} &= n^{-1}\\sigma^2_{\\epsilon}  \\left(n^{-1}\\mathbf{x}'\\mathbf{x}\\right)^{-1} \\\\\n% ---------\n&= \\frac{\\sigma^2_{\\epsilon}}{n}  \\left(\\frac{1}{\\hat\\sigma^2_x} \\begin{bmatrix}\n\\bar{x^2} & -\\bar{x}\\\\\n-\\bar{x} & 1  \n\\end{bmatrix}\n\\right)\n\\end{align}\n\\]\nThe variance for \\(\\hat\\beta_0\\) corresponds to the first diagonal element of \\({\\bm{\\Sigma}}\\). Multiplying the relevant terms gets us\n\\[\n\\begin{align}\n\\text{Var}[\\hat\\beta_0] &= \\frac{\\sigma^2_{\\epsilon}}{n} \\cdot \\frac{1}{\\hat\\sigma^2_x} \\cdot \\bar{x^2} \\\\\n% ---------\n&= \\frac{\\sigma^2_{\\epsilon}\\bar{x^2} }{n \\hat\\sigma^2_x} \\\\\n% ---------\n&= \\frac{\\sigma^2_{\\epsilon}( \\bar{x}^2  + \\hat\\sigma^2_x )}{n \\hat\\sigma^2_x} \\\\\n% ---------\n&= \\frac{\\sigma^2_{\\epsilon}\\bar{x}^2  + \\sigma^2_{\\epsilon}\\hat\\sigma^2_x}{n \\hat\\sigma^2_x} \\\\\n% ---------\n&= \\frac{\\sigma^2_{\\epsilon}\\bar{x}^2}{n \\hat\\sigma^2_x} + \\frac{\\sigma^2_{\\epsilon}\\hat\\sigma^2_x}{n \\hat\\sigma^2_x} \\\\\n% ---------\n&= \\frac{\\sigma^2_{\\epsilon}}{n} \\left(\n  \\frac{\\bar{x}^2}{\\sigma^2_x} + 1\n\\right).\n\\end{align}\n\\]\nThe variance for \\(\\hat\\beta_1\\) corresponds to the second diagonal element of \\({\\bm{\\Sigma}}\\), and again multiplying the relevant terms\n\\[\n\\begin{align}\n\\text{Var}[\\hat\\beta_1] &= \\frac{\\sigma^2_{\\epsilon}}{n} \\cdot \\frac{1}{\\hat\\sigma^2_x} \\cdot 1 \\\\\n% ---------\n&= \\frac{\\sigma^2_{\\epsilon}}{n} \\cdot \\frac{1}{\\frac{\\sum (x_i - \\bar{x})^2}{n}}\\\\\n% ---------\n&= \\frac{\\sigma^2_{\\epsilon}}{\\frac{n\\sum (x_i - \\bar{x})^2}{n}}\\\\\n% ---------\n&= \\frac{\\sigma^2_{\\epsilon}}{\\sum_{i=1}^n\\left(x_i - \\bar{x}\\right)^2}.\n\\end{align}\n\\]\nNice! With that, we connected the variances for the regression parameters obtained via the matrix form of OLS to those obtained through scalar notation."
  },
  {
    "objectID": "posts/2024-11-16-matrix-ols/index.html#footnotes",
    "href": "posts/2024-11-16-matrix-ols/index.html#footnotes",
    "title": "Connecting Scalar and Matrix Notation in OLS",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis will be handy in letting us define the elements in the relevant vectors and matrices as means and variances.‚Ü©Ô∏é\nThe inverse of a \\(2 \\times 2\\) matrix is given by \\(\\left[\\begin{smallmatrix}a & b \\\\ c & d\\end{smallmatrix}\\right]^{-1} = \\frac{1}{ad-bc}\\left[\\begin{smallmatrix}d & -b \\\\ -c & a\\end{smallmatrix}\\right]\\).‚Ü©Ô∏é\nThis stems from the fact that we defined \\(\\hat{\\bm{\\beta}} = \\bm{\\beta} + \\left(\\mathbf{x}'\\mathbf{x}\\right)^{-1}\\mathbf{x}'\\bm{\\epsilon}\\) and the normality of \\(\\bm{\\epsilon}\\) is preserved under affine transformations.‚Ü©Ô∏é"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "Hi! My name is Josue (pronounced ho-sway).\nI‚Äôm a data scientist at McGraw Hill where I‚Äôm helping build personalized learning experiences at scale. I previously earned a PhD in Quantitative Psychology at UC Davis, where I focused on developing Bayesian statistical models for social-behavior science."
  },
  {
    "objectID": "about.html#work",
    "href": "about.html#work",
    "title": "about",
    "section": "",
    "text": "Hi! My name is Josue (pronounced ho-sway).\nI‚Äôm a data scientist at McGraw Hill where I‚Äôm helping build personalized learning experiences at scale. I previously earned a PhD in Quantitative Psychology at UC Davis, where I focused on developing Bayesian statistical models for social-behavior science."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "about",
    "section": " education",
    "text": "education\n\n\n\n  \n    \n      PhD in Quantitative Psychology ‚Äî\n      2023\n    \n    University of California, Davis\n  \n  \n  \n    \n      MA in Cognitive Psychology ‚Äî\n      2019\n    \n    Minor in Statistics\n    California State Polytechnic University, Humboldt\n  \n  \n  \n  \n    \n      BA in Psychology ‚Äî\n      2017\n    \n    California State Polytechnic University, Humboldt"
  }
]